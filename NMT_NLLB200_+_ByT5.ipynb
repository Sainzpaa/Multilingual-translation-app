{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85216c0b7d4c495c99b61b8c5a822fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e36fcc0bbaf43a69659938b29c5add9",
              "IPY_MODEL_bddece4f8df944d5954516d21841d840",
              "IPY_MODEL_627829a0e4934cd9ad862c3dab9a0a66"
            ],
            "layout": "IPY_MODEL_cbbec8a339f94399b271100d3032e8a9"
          }
        },
        "0e36fcc0bbaf43a69659938b29c5add9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec1f9314df84d82b1259c59165e3d59",
            "placeholder": "​",
            "style": "IPY_MODEL_ff25cb3c37c04f089178674ab95aebe4",
            "value": "Epoch 1/10:   3%"
          }
        },
        "bddece4f8df944d5954516d21841d840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc3d2e0ffdbb4903b888ce2ed84bc38e",
            "max": 6250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34949c49330b4c58a346e572deb91211",
            "value": 188
          }
        },
        "627829a0e4934cd9ad862c3dab9a0a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d6ddc4b06c45298e4dbd8f2c64266a",
            "placeholder": "​",
            "style": "IPY_MODEL_3614f9036c9442c298c6eff0c1c53042",
            "value": " 188/6250 [46:38&lt;25:07:51, 14.92s/it, loss=5.72, lr=1e-5, use_nllb=1, epoch=1/10]"
          }
        },
        "cbbec8a339f94399b271100d3032e8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec1f9314df84d82b1259c59165e3d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff25cb3c37c04f089178674ab95aebe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3d2e0ffdbb4903b888ce2ed84bc38e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34949c49330b4c58a346e572deb91211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68d6ddc4b06c45298e4dbd8f2c64266a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3614f9036c9442c298c6eff0c1c53042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Celda 1: Instalación de dependencias"
      ],
      "metadata": {
        "id": "vtfT5LYiZhj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets sentencepiece sacrebleu torch accelerate\n",
        "!pip install protobuf==3.20.* --force-reinstall\n",
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GdIPBEzrZjvw",
        "outputId": "7491da99-f0ba-4205-b6a3-8b3c5ff6988a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "755234cf192b439fab288afc53729a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 2: Importaciones necesarias"
      ],
      "metadata": {
        "id": "0RiyFwGRZjRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F\n",
        "  from torch.utils.data import Dataset, DataLoader\n",
        "  from transformers import (\n",
        "      NllbTokenizer,\n",
        "      AutoModelForSeq2SeqLM,\n",
        "      ByT5Tokenizer,\n",
        "      T5ForConditionalGeneration,\n",
        "      get_linear_schedule_with_warmup,\n",
        "      AutoTokenizer\n",
        "  )\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from typing import List, Dict, Tuple, Optional\n",
        "  import json\n",
        "  import os\n",
        "  from tqdm.auto import tqdm\n",
        "  import gc\n",
        "  import warnings\n",
        "\n",
        "  # Configurar warnings y determinismo\n",
        "  warnings.filterwarnings('ignore')\n",
        "\n",
        "  # Configuración para evitar NaN y problemas de estabilidad\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.manual_seed(42)\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Configuración adicional para debugging CUDA\n",
        "  os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Para debug CUDA\n",
        "  os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"    # Para device-side assertions\n",
        "\n",
        "  # Verificar GPU y configurar device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(f\"🔧 Using device: {device}\")\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "      print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "      # Limpiar caché CUDA al inicio\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      # Mostrar memoria disponible\n",
        "      memory_free = torch.cuda.mem_get_info()[0] / 1e9\n",
        "      memory_total = torch.cuda.mem_get_info()[1] / 1e9\n",
        "      print(f\"💾 GPU Memory Available: {memory_free:.2f} GB / {memory_total:.2f} GB\")\n",
        "\n",
        "      # Configurar para evitar fragmentación de memoria\n",
        "      torch.cuda.set_per_process_memory_fraction(0.8)  # Usar solo 80% de la GPU\n",
        "\n",
        "  else:\n",
        "      print(\"⚠️ CUDA no disponible - usando CPU (será más lento)\")\n",
        "\n",
        "  # Configuración de PyTorch para estabilidad numérica\n",
        "  torch.set_default_dtype(torch.float32)\n",
        "\n",
        "  # Funciones de utilidad para debugging\n",
        "  def check_tensor_health(tensor, name=\"tensor\"):\n",
        "      \"\"\"Verifica si un tensor tiene valores problemáticos\"\"\"\n",
        "      if torch.isnan(tensor).any():\n",
        "          print(f\"⚠️ NaN detectado en {name}\")\n",
        "          return False\n",
        "      if torch.isinf(tensor).any():\n",
        "          print(f\"⚠️ Inf detectado en {name}\")\n",
        "          return False\n",
        "      return True\n",
        "\n",
        "  def print_memory_usage():\n",
        "      \"\"\"Imprime uso actual de memoria\"\"\"\n",
        "      if torch.cuda.is_available():\n",
        "          allocated = torch.cuda.memory_allocated() / 1e9\n",
        "          cached = torch.cuda.memory_reserved() / 1e9\n",
        "          print(f\"💾 GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
        "\n",
        "  def cleanup_memory():\n",
        "      \"\"\"Limpia memoria GPU\"\"\"\n",
        "      if torch.cuda.is_available():\n",
        "          torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "  print(\"✅ Importaciones y configuración inicial completada\")\n",
        "  print(\"🔧 Configuración de estabilidad numérica activada\")\n",
        "  print(\"🧹 Limpieza automática de memoria configurada\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOuamdcjZnLA",
        "outputId": "03099c90-8549-4f4d-bc7b-4074d71f661c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Using device: cpu\n",
            "⚠️ CUDA no disponible - usando CPU (será más lento)\n",
            "✅ Importaciones y configuración inicial completada\n",
            "🔧 Configuración de estabilidad numérica activada\n",
            "🧹 Limpieza automática de memoria configurada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 3: Configuración del modelo híbrido con idiomas adicionales"
      ],
      "metadata": {
        "id": "SPeqeUm8ZpwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  class HybridTranslationConfig:\n",
        "      def __init__(self):\n",
        "          # Modelos base\n",
        "          self.nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "          self.byt5_model_name = \"google/byt5-small\"\n",
        "\n",
        "          # CONFIGURACIÓN OPTIMIZADA: ÉPOCAS MÁS CORTAS\n",
        "          self.batch_size = 8\n",
        "          self.learning_rate = 1e-5\n",
        "          self.num_epochs = 10           # ← MÁS ÉPOCAS\n",
        "          self.max_length = 64\n",
        "          self.warmup_steps = 100\n",
        "          self.gradient_accumulation_steps = 4\n",
        "\n",
        "          # LÍMITES POR ÉPOCA (CLAVE PARA ÉPOCAS CORTAS)\n",
        "          self.max_samples_per_epoch = 50000    # ← SOLO 50K samples por época\n",
        "          self.early_stopping_patience = 3     # ← Para en 3 épocas sin mejora\n",
        "          self.save_every_n_epochs = 1         # ← Guardar cada época\n",
        "\n",
        "          # Configuración del modelo híbrido\n",
        "          self.hidden_size = 512\n",
        "          self.fusion_dropout = 0.1\n",
        "          self.temperature = 1.0\n",
        "\n",
        "          # Learning rate scheduling mejorado\n",
        "          self.lr_scheduler_type = \"cosine\"     # ← Cosine annealing\n",
        "          self.min_lr = 1e-7                   # ← LR mínimo\n",
        "\n",
        "          # IDIOMAS SOPORTADOS POR NLLB-200\n",
        "          self.nllb_languages = [\n",
        "              # Idiomas principales\n",
        "              'eng_Latn', 'spa_Latn', 'fra_Latn', 'deu_Latn', 'ita_Latn',\n",
        "              'por_Latn', 'rus_Cyrl', 'zho_Hans', 'jpn_Jpan', 'kor_Hang',\n",
        "              'ara_Arab', 'hin_Deva', 'tur_Latn', 'pol_Latn', 'nld_Latn',\n",
        "\n",
        "              # IDIOMAS AFRICANOS INCLUIDOS EN NLLB-200\n",
        "              'wol_Latn',    # Wolof (Senegal, Gambia)\n",
        "              'swh_Latn',    # Swahili (Tanzania, Kenya, Uganda)\n",
        "              'amh_Ethi',    # Amhárico (Etiopía)\n",
        "              'hau_Latn',    # Hausa (Nigeria, Níger)\n",
        "              'ibo_Latn',    # Igbo (Nigeria)\n",
        "              'yor_Latn',    # Yoruba (Nigeria, Benín)\n",
        "              'sna_Latn',    # Shona (Zimbabwe)\n",
        "              'som_Latn',    # Somalí (Somalia, Etiopía)\n",
        "              'afr_Latn',    # Afrikáans (Sudáfrica)\n",
        "              'xho_Latn',    # Xhosa (Sudáfrica)\n",
        "              'zul_Latn',    # Zulu (Sudáfrica)\n",
        "              'tsn_Latn',    # Tswana (Botsuana)\n",
        "              'nso_Latn',    # Sotho del Norte (Sudáfrica)\n",
        "              'ven_Latn',    # Venda (Sudáfrica)\n",
        "              'tso_Latn',    # Tsonga (Sudáfrica)\n",
        "              'ssw_Latn',    # Siswati (Esuatini)\n",
        "              'lug_Latn',    # Luganda (Uganda)\n",
        "              'kik_Latn',    # Kikuyu (Kenya)\n",
        "              'luo_Latn',    # Luo (Kenya, Uganda)\n",
        "              'rny_Latn',    # Runyanakore (Uganda)\n",
        "              'lgg_Latn',    # Lugbara (Uganda, RDC)\n",
        "              'fon_Latn',    # Fon (Benín)\n",
        "              'twi_Latn',    # Twi (Ghana)\n",
        "              'aka_Latn',    # Akan (Ghana)\n",
        "              'bam_Latn',    # Bambara (Mali)\n",
        "              'dyu_Latn',    # Dyula (Costa de Marfil)\n",
        "              'mos_Latn',    # Mossi (Burkina Faso)\n",
        "              'fuv_Latn',    # Fulfulde (Nigeria, otros países)\n",
        "\n",
        "              # UZBEKO Y OTROS IDIOMAS ASIÁTICOS\n",
        "              'uzn_Latn',    # Uzbeko (Uzbekistán)\n",
        "              'kaz_Cyrl',    # Kazajo (Kazajistán)\n",
        "              'kir_Cyrl',    # Kirguís (Kirguistán)\n",
        "              'tgk_Cyrl',    # Tayiko (Tayikistán)\n",
        "              'tuk_Latn',    # Turkmeno (Turkmenistán)\n",
        "              'aze_Latn',    # Azerbaiyano (Azerbaiyán)\n",
        "\n",
        "              # OTROS IDIOMAS ÚTILES\n",
        "              'ben_Beng',    # Bengalí\n",
        "              'urd_Arab',    # Urdu\n",
        "              'fas_Arab',    # Persa/Farsi\n",
        "              'mya_Mymr',    # Birmano\n",
        "              'tha_Thai',    # Tailandés\n",
        "              'vie_Latn',    # Vietnamita\n",
        "              'ind_Latn',    # Indonesio\n",
        "              'msa_Latn',    # Malayo\n",
        "              'tgl_Latn',    # Tagalo (Filipinas)\n",
        "              'ceb_Latn',    # Cebuano (Filipinas)\n",
        "          ]\n",
        "\n",
        "          # Nuevos idiomas para entrenar con ByT5\n",
        "          self.new_languages = []\n",
        "\n",
        "          # MAPEO DE CÓDIGOS PERSONALIZADOS A NLLB\n",
        "          self.language_mapping = {\n",
        "              # Códigos de tus datos → Códigos NLLB\n",
        "              'EN': 'eng_Latn', 'ES': 'spa_Latn', 'FR': 'fra_Latn',\n",
        "              'DE': 'deu_Latn', 'IT': 'ita_Latn', 'PT': 'por_Latn',\n",
        "              'RU': 'rus_Cyrl', 'AR': 'ara_Arab', 'ZH': 'zho_Hans',\n",
        "              'JA': 'jpn_Jpan', 'KO': 'kor_Hang',\n",
        "\n",
        "              # Idiomas africanos\n",
        "              'WO': 'wol_Latn',    # Wolof\n",
        "              'SW': 'swh_Latn',    # Swahili\n",
        "              'AM': 'amh_Ethi',    # Amhárico\n",
        "              'HA': 'hau_Latn',    # Hausa\n",
        "              'IG': 'ibo_Latn',    # Igbo\n",
        "              'YO': 'yor_Latn',    # Yoruba\n",
        "              'SN': 'sna_Latn',    # Shona\n",
        "              'SO': 'som_Latn',    # Somalí\n",
        "              'AF': 'afr_Latn',    # Afrikáans\n",
        "              'XH': 'xho_Latn',    # Xhosa\n",
        "              'ZU': 'zul_Latn',    # Zulu\n",
        "              'TW': 'twi_Latn',    # Twi\n",
        "              'AK': 'aka_Latn',    # Akan\n",
        "              'BM': 'bam_Latn',    # Bambara\n",
        "\n",
        "              # Uzbeko y otros asiáticos\n",
        "              'UZ': 'uzn_Latn',    # Uzbeko\n",
        "              'KK': 'kaz_Cyrl',    # Kazajo\n",
        "              'KY': 'kir_Cyrl',    # Kirguís\n",
        "              'TG': 'tgk_Cyrl',    # Tayiko\n",
        "              'TK': 'tuk_Latn',    # Turkmeno\n",
        "              'AZ': 'aze_Latn',    # Azerbaiyano\n",
        "\n",
        "              # Otros útiles\n",
        "              'BN': 'ben_Beng',    # Bengalí\n",
        "              'UR': 'urd_Arab',    # Urdu\n",
        "              'FA': 'fas_Arab',    # Persa\n",
        "              'MY': 'mya_Mymr',    # Birmano\n",
        "              'TH': 'tha_Thai',    # Tailandés\n",
        "              'VI': 'vie_Latn',    # Vietnamita\n",
        "              'ID': 'ind_Latn',    # Indonesio\n",
        "              'MS': 'msa_Latn',    # Malayo\n",
        "              'TL': 'tgl_Latn',    # Tagalo\n",
        "          }\n",
        "\n",
        "      def get_nllb_code(self, custom_code):\n",
        "          \"\"\"Convierte código personalizado a código NLLB\"\"\"\n",
        "          return self.language_mapping.get(custom_code.upper(), custom_code)\n",
        "\n",
        "      def is_supported_by_nllb(self, lang_code):\n",
        "          \"\"\"Verifica si un idioma está soportado por NLLB\"\"\"\n",
        "          nllb_code = self.get_nllb_code(lang_code)\n",
        "          return nllb_code in self.nllb_languages\n",
        "\n",
        "  config = HybridTranslationConfig()\n",
        "\n",
        "  print(\"🔧 CONFIGURACIÓN OPTIMIZADA:\")\n",
        "  print(f\"  • Épocas: {config.num_epochs} (más épocas)\")\n",
        "  print(f\"  • Samples por época: {config.max_samples_per_epoch:,} (épocas más cortas)\")\n",
        "  print(f\"  • Tiempo estimado por época: ~45-60 minutos\")\n",
        "  print(f\"  • Total estimado: ~8-10 horas\")\n",
        "  print(f\"  • Guardado: Cada {config.save_every_n_epochs} época(s)\")\n",
        "  print(f\"  • Early stopping: {config.early_stopping_patience} épocas sin mejora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS6obzNcZqAv",
        "outputId": "1c1f588c-a358-4e2c-ef78-03f81f3dd424"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 CONFIGURACIÓN OPTIMIZADA:\n",
            "  • Épocas: 10 (más épocas)\n",
            "  • Samples por época: 50,000 (épocas más cortas)\n",
            "  • Tiempo estimado por época: ~45-60 minutos\n",
            "  • Total estimado: ~8-10 horas\n",
            "  • Guardado: Cada 1 época(s)\n",
            "  • Early stopping: 3 épocas sin mejora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 4: Clase del modelo híbrido NLLB + ByT5"
      ],
      "metadata": {
        "id": "MurSiMNyZttv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  class HybridNLLBByT5Model(nn.Module):\n",
        "      def __init__(self, config):\n",
        "          super().__init__()\n",
        "          self.config = config\n",
        "\n",
        "          # Cargar modelos pre-entrenados\n",
        "          print(\"Loading NLLB model...\")\n",
        "          self.nllb_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "              config.nllb_model_name,\n",
        "              torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "          )\n",
        "          self.nllb_tokenizer = AutoTokenizer.from_pretrained(config.nllb_model_name)\n",
        "\n",
        "          print(\"Loading ByT5 model...\")\n",
        "          self.byt5_model = T5ForConditionalGeneration.from_pretrained(config.byt5_model_name)\n",
        "          self.byt5_tokenizer = ByT5Tokenizer.from_pretrained(config.byt5_model_name)\n",
        "\n",
        "          # Obtener dimensiones de hidden states\n",
        "          nllb_hidden_size = self.nllb_model.config.hidden_size\n",
        "          byt5_hidden_size = self.byt5_model.config.d_model\n",
        "          byt5_vocab_size = self.byt5_model.config.vocab_size\n",
        "\n",
        "          print(f\"NLLB hidden size: {nllb_hidden_size}\")\n",
        "          print(f\"ByT5 hidden size: {byt5_hidden_size}\")\n",
        "          print(f\"ByT5 vocab size: {byt5_vocab_size}\")\n",
        "\n",
        "          # CORRECCIÓN: Usar solo ByT5 para datos tokenizados con ByT5\n",
        "          # El modelo híbrido se simplifica para evitar incompatibilidades\n",
        "\n",
        "          # Capa de adaptación para mejorar ByT5\n",
        "          self.adaptation_layer = nn.Sequential(\n",
        "              nn.Linear(byt5_hidden_size, config.hidden_size),\n",
        "              nn.LayerNorm(config.hidden_size),\n",
        "              nn.ReLU(),\n",
        "              nn.Dropout(config.fusion_dropout),\n",
        "              nn.Linear(config.hidden_size, byt5_hidden_size)\n",
        "          )\n",
        "\n",
        "          # Proyección mejorada para salida\n",
        "          self.enhanced_output = nn.Sequential(\n",
        "              nn.Linear(byt5_hidden_size, byt5_hidden_size * 2),\n",
        "              nn.GELU(),\n",
        "              nn.Dropout(config.fusion_dropout),\n",
        "              nn.Linear(byt5_hidden_size * 2, byt5_vocab_size)\n",
        "          )\n",
        "\n",
        "          # Inicializar pesos correctamente\n",
        "          self._init_weights()\n",
        "\n",
        "      def _init_weights(self):\n",
        "          \"\"\"Inicializa pesos con valores pequeños para evitar NaN\"\"\"\n",
        "          for module in [self.adaptation_layer, self.enhanced_output]:\n",
        "              for layer in module:\n",
        "                  if isinstance(layer, nn.Linear):\n",
        "                      # Inicialización Xavier con valores más pequeños\n",
        "                      torch.nn.init.xavier_normal_(layer.weight, gain=0.1)\n",
        "                      if layer.bias is not None:\n",
        "                          torch.nn.init.zeros_(layer.bias)\n",
        "\n",
        "      def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
        "          \"\"\"Forward pass simplificado usando solo ByT5 con mejoras\"\"\"\n",
        "\n",
        "          # Validar entrada\n",
        "          if input_ids.max().item() >= self.byt5_model.config.vocab_size:\n",
        "              print(f\"⚠️ Token fuera de rango detectado: max={input_ids.max().item()}, vocab_size={self.byt5_model.config.vocab_size}\")\n",
        "              # Clamp tokens para evitar errores\n",
        "              input_ids = torch.clamp(input_ids, 0, self.byt5_model.config.vocab_size - 1)\n",
        "\n",
        "          if labels is not None and labels.max().item() >= self.byt5_model.config.vocab_size:\n",
        "              # Ignorar tokens fuera de rango en labels\n",
        "              labels = torch.where(\n",
        "                  labels >= self.byt5_model.config.vocab_size,\n",
        "                  torch.tensor(-100, device=labels.device),\n",
        "                  labels\n",
        "              )\n",
        "\n",
        "          try:\n",
        "              # Usar solo ByT5 (más estable para datos tokenizados con ByT5)\n",
        "              outputs = self.byt5_model(\n",
        "                  input_ids=input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  labels=labels,\n",
        "                  output_hidden_states=True\n",
        "              )\n",
        "\n",
        "              # Obtener hidden states\n",
        "              encoder_hidden_states = outputs.encoder_last_hidden_state\n",
        "\n",
        "              # Aplicar capa de adaptación\n",
        "              adapted_hidden = self.adaptation_layer(encoder_hidden_states)\n",
        "\n",
        "              # Skip connection\n",
        "              enhanced_hidden = encoder_hidden_states + adapted_hidden\n",
        "\n",
        "              # Proyección mejorada para logits\n",
        "              enhanced_logits = self.enhanced_output(enhanced_hidden)\n",
        "\n",
        "              # Calcular pérdida si hay labels\n",
        "              loss = None\n",
        "              if labels is not None:\n",
        "                  # Usar solo logits válidos\n",
        "                  shift_logits = enhanced_logits[..., :-1, :].contiguous()\n",
        "                  shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "                  loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
        "                  loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "                  # Verificar que no sea NaN\n",
        "                  if torch.isnan(loss):\n",
        "                      print(\"⚠️ NaN loss detectado, usando loss por defecto\")\n",
        "                      loss = torch.tensor(0.0, device=loss.device, requires_grad=True)\n",
        "\n",
        "              return {\n",
        "                  'loss': loss,\n",
        "                  'logits': enhanced_logits,\n",
        "                  'hidden_states': enhanced_hidden\n",
        "              }\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"❌ Error en forward pass: {e}\")\n",
        "              # Devolver loss válido para evitar crash\n",
        "              dummy_loss = torch.tensor(0.0, device=input_ids.device, requires_grad=True)\n",
        "              dummy_logits = torch.zeros(\n",
        "                  input_ids.shape[0], input_ids.shape[1], self.byt5_model.config.vocab_size,\n",
        "                  device=input_ids.device\n",
        "              )\n",
        "              return {\n",
        "                  'loss': dummy_loss,\n",
        "                  'logits': dummy_logits,\n",
        "                  'hidden_states': None\n",
        "              }\n",
        "\n",
        "      def generate_translation(self, text, src_lang, tgt_lang, max_length=256):\n",
        "          \"\"\"Genera traducción usando ByT5 mejorado\"\"\"\n",
        "          self.eval()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # Usar solo ByT5 para generación\n",
        "              inputs = self.byt5_tokenizer(text, return_tensors=\"pt\",\n",
        "                                          max_length=max_length, truncation=True)\n",
        "              inputs = {k: v.to(next(self.parameters()).device) for k, v in inputs.items()}\n",
        "\n",
        "              try:\n",
        "                  generated_tokens = self.byt5_model.generate(\n",
        "                      **inputs,\n",
        "                      max_length=max_length,\n",
        "                      num_beams=2,\n",
        "                      temperature=0.8,\n",
        "                      do_sample=True,\n",
        "                      pad_token_id=self.byt5_tokenizer.pad_token_id\n",
        "                  )\n",
        "\n",
        "                  translation = self.byt5_tokenizer.decode(generated_tokens[0],\n",
        "                                                         skip_special_tokens=True)\n",
        "                  return translation\n",
        "\n",
        "              except Exception as e:\n",
        "                  print(f\"Error en generación: {e}\")\n",
        "                  return f\"Error generating translation for: {text}\"\n"
      ],
      "metadata": {
        "id": "fpyj8udnZtSo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 5: Dataset para datos PRE-TOKENIZADOS con ByT5"
      ],
      "metadata": {
        "id": "BNRR02aAZwXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  from torch.utils.data import Dataset, DataLoader\n",
        "  import torch\n",
        "\n",
        "  class PreTokenizedTranslationDataset(Dataset):\n",
        "      \"\"\"Dataset para datos ya tokenizados con ByT5Tokenizer\"\"\"\n",
        "\n",
        "      def __init__(self, tokenized_pairs, max_length=256):\n",
        "          \"\"\"\n",
        "          tokenized_pairs: Lista de diccionarios con:\n",
        "          - input_ids: tensor de tokens de entrada\n",
        "          - attention_mask: tensor de máscaras de atención\n",
        "          - labels: tensor de tokens objetivo\n",
        "          - target_attention_mask: tensor de máscaras objetivo\n",
        "          - src_lang: idioma fuente\n",
        "          - tgt_lang: idioma destino\n",
        "          \"\"\"\n",
        "          self.tokenized_pairs = tokenized_pairs\n",
        "          self.max_length = max_length\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.tokenized_pairs)\n",
        "\n",
        "      def pad_or_truncate(self, tensor, max_length, pad_token_id=0):\n",
        "          \"\"\"Trunca o hace padding a tensor\"\"\"\n",
        "          if len(tensor) > max_length:\n",
        "              return tensor[:max_length]\n",
        "          elif len(tensor) < max_length:\n",
        "              padding = torch.full((max_length - len(tensor),), pad_token_id, dtype=tensor.dtype)\n",
        "              return torch.cat([tensor, padding])\n",
        "          else:\n",
        "              return tensor\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          item = self.tokenized_pairs[idx]\n",
        "\n",
        "          # Obtener tensores\n",
        "          input_ids = item['input_ids']\n",
        "          attention_mask = item['attention_mask']\n",
        "          labels = item['labels']\n",
        "          target_attention_mask = item['target_attention_mask']\n",
        "\n",
        "          # Aplicar padding/truncation\n",
        "          input_ids = self.pad_or_truncate(input_ids, self.max_length, pad_token_id=0)\n",
        "          attention_mask = self.pad_or_truncate(attention_mask, self.max_length, pad_token_id=0)\n",
        "          labels = self.pad_or_truncate(labels, self.max_length, pad_token_id=-100)  # -100 para ignorar en loss\n",
        "          target_attention_mask = self.pad_or_truncate(target_attention_mask, self.max_length, pad_token_id=0)\n",
        "\n",
        "          return {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': attention_mask,\n",
        "              'labels': labels,\n",
        "              'target_attention_mask': target_attention_mask,\n",
        "              'src_lang': item['src_lang'],\n",
        "              'tgt_lang': item['tgt_lang']\n",
        "          }\n",
        "\n",
        "  def create_tokenized_data_collator():\n",
        "      \"\"\"Collator para datos pre-tokenizados\"\"\"\n",
        "      def collate_fn(batch):\n",
        "          # Apilar tensores\n",
        "          input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "          attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "          labels = torch.stack([item['labels'] for item in batch])\n",
        "          target_attention_mask = torch.stack([item['target_attention_mask'] for item in batch])\n",
        "\n",
        "          # Idiomas\n",
        "          src_langs = [item['src_lang'] for item in batch]\n",
        "          tgt_langs = [item['tgt_lang'] for item in batch]\n",
        "\n",
        "          return {\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': attention_mask,\n",
        "              'labels': labels,\n",
        "              'target_attention_mask': target_attention_mask,\n",
        "              'src_langs': src_langs,\n",
        "              'tgt_langs': tgt_langs\n",
        "          }\n",
        "\n",
        "      return collate_fn\n",
        "\n",
        "  # Función helper para crear dataloaders\n",
        "  def create_tokenized_dataloaders(train_pairs, val_pairs, batch_size=4, max_length=256):\n",
        "      \"\"\"Crea dataloaders para datos pre-tokenizados\"\"\"\n",
        "\n",
        "      # Crear datasets\n",
        "      train_dataset = PreTokenizedTranslationDataset(train_pairs, max_length)\n",
        "      val_dataset = PreTokenizedTranslationDataset(val_pairs, max_length)\n",
        "\n",
        "      # Crear collator\n",
        "      collator = create_tokenized_data_collator()\n",
        "\n",
        "      # Crear dataloaders\n",
        "      train_loader = DataLoader(\n",
        "          train_dataset,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True,\n",
        "          collate_fn=collator,\n",
        "          num_workers=0,  # 0 para evitar problemas en Colab\n",
        "          pin_memory=torch.cuda.is_available()\n",
        "      )\n",
        "\n",
        "      val_loader = DataLoader(\n",
        "          val_dataset,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=False,\n",
        "          collate_fn=collator,\n",
        "          num_workers=0,\n",
        "          pin_memory=torch.cuda.is_available()\n",
        "      )\n",
        "\n",
        "      print(f\"✅ Dataloaders creados:\")\n",
        "      print(f\"  • Train batches: {len(train_loader)}\")\n",
        "      print(f\"  • Val batches: {len(val_loader)}\")\n",
        "      print(f\"  • Batch size: {batch_size}\")\n",
        "      print(f\"  • Max length: {max_length}\")\n",
        "\n",
        "      return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "MLQLpgR7ZwqO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 6: Carga de datos PRE-TOKENIZADOS desde Google Drive\n",
        "\n"
      ],
      "metadata": {
        "id": "c5wluadHZ0-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  from google.colab import drive\n",
        "  import pandas as pd\n",
        "  import os\n",
        "  import glob\n",
        "  import gc\n",
        "  import torch\n",
        "  import ast\n",
        "  import json\n",
        "  import random\n",
        "\n",
        "  def mount_drive_and_setup_paths():\n",
        "      \"\"\"Monta Google Drive y configura rutas\"\"\"\n",
        "      print(\"🔌 Montando Google Drive...\")\n",
        "      drive.mount('/content/drive')\n",
        "      print(\"✅ Google Drive montado exitosamente\")\n",
        "\n",
        "      base_path = \"/content/drive/MyDrive/GlobalTranslator/NMT/\"\n",
        "      dataset_path = base_path + \"Dataset/\"\n",
        "      models_path = base_path + \"Models/\"\n",
        "\n",
        "      os.makedirs(dataset_path, exist_ok=True)\n",
        "      os.makedirs(models_path, exist_ok=True)\n",
        "\n",
        "      print(f\"📂 Dataset path: {dataset_path}\")\n",
        "      print(f\"💾 Models path: {models_path}\")\n",
        "\n",
        "      return dataset_path, models_path\n",
        "\n",
        "  def parse_tokenized_data(data_string):\n",
        "      \"\"\"Convierte string de lista a tensor de PyTorch\"\"\"\n",
        "      try:\n",
        "          if isinstance(data_string, str):\n",
        "              if data_string.startswith('\"') and data_string.endswith('\"'):\n",
        "                  data_string = data_string[1:-1]\n",
        "              token_list = ast.literal_eval(data_string)\n",
        "          else:\n",
        "              token_list = data_string\n",
        "\n",
        "          return torch.tensor(token_list, dtype=torch.long)\n",
        "      except Exception as e:\n",
        "          print(f\"Error parseando tokens: {e}\")\n",
        "          return torch.tensor([1], dtype=torch.long)\n",
        "\n",
        "  def load_tokenized_csv(file_path, max_samples=None):\n",
        "      \"\"\"Carga CSV con datos pre-tokenizados\"\"\"\n",
        "\n",
        "      print(f\"📥 Cargando {os.path.basename(file_path)}...\")\n",
        "\n",
        "      try:\n",
        "          df = pd.read_csv(file_path)\n",
        "\n",
        "          expected_cols = ['input_ids', 'input_attention_mask', 'target_ids', 'target_attention_mask', 'input_label', 'target_label']\n",
        "          missing_cols = [col for col in expected_cols if col not in df.columns]\n",
        "\n",
        "          if missing_cols:\n",
        "              print(f\"  ❌ Columnas faltantes: {missing_cols}\")\n",
        "              return []\n",
        "\n",
        "          print(f\"  📊 Filas encontradas: {len(df)}\")\n",
        "\n",
        "          if max_samples and len(df) > max_samples:\n",
        "              df = df.head(max_samples)\n",
        "              print(f\"  ✂️ Limitado a: {len(df)} samples\")\n",
        "\n",
        "          tokenized_pairs = []\n",
        "          errors = 0\n",
        "\n",
        "          for idx, row in df.iterrows():\n",
        "              try:\n",
        "                  input_ids = parse_tokenized_data(row['input_ids'])\n",
        "                  input_attention_mask = parse_tokenized_data(row['input_attention_mask'])\n",
        "                  target_ids = parse_tokenized_data(row['target_ids'])\n",
        "                  target_attention_mask = parse_tokenized_data(row['target_attention_mask'])\n",
        "\n",
        "                  src_lang = str(row['input_label']).strip()\n",
        "                  tgt_lang = str(row['target_label']).strip()\n",
        "\n",
        "                  if len(input_ids) < 2 or len(target_ids) < 2:\n",
        "                      continue\n",
        "\n",
        "                  if len(input_ids) != len(input_attention_mask):\n",
        "                      continue\n",
        "\n",
        "                  if len(target_ids) != len(target_attention_mask):\n",
        "                      continue\n",
        "\n",
        "                  tokenized_pairs.append({\n",
        "                      'input_ids': input_ids,\n",
        "                      'attention_mask': input_attention_mask,\n",
        "                      'labels': target_ids,\n",
        "                      'target_attention_mask': target_attention_mask,\n",
        "                      'src_lang': src_lang,\n",
        "                      'tgt_lang': tgt_lang\n",
        "                  })\n",
        "\n",
        "              except Exception as e:\n",
        "                  errors += 1\n",
        "                  if errors < 5:\n",
        "                      print(f\"  ⚠️ Error en fila {idx}: {str(e)[:50]}...\")\n",
        "                  continue\n",
        "\n",
        "          if errors > 0:\n",
        "              print(f\"  ⚠️ Total errores: {errors}\")\n",
        "\n",
        "          print(f\"  ✅ Pares válidos: {len(tokenized_pairs)}\")\n",
        "          return tokenized_pairs\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"  ❌ Error cargando archivo: {e}\")\n",
        "          return []\n",
        "\n",
        "  def load_fragmented_tokenized_data(dataset_path):\n",
        "      \"\"\"Carga todos los archivos CSV tokenizados\"\"\"\n",
        "\n",
        "      train_files = sorted(glob.glob(dataset_path + \"NMT_train*.csv\"))\n",
        "      val_files = sorted(glob.glob(dataset_path + \"NMT_val*.csv\"))\n",
        "\n",
        "      print(f\"\\n📁 ARCHIVOS TOKENIZADOS ENCONTRADOS:\")\n",
        "      print(f\"  • Training files: {len(train_files)}\")\n",
        "      total_size_mb = 0\n",
        "      for f in train_files:\n",
        "          size_mb = os.path.getsize(f) / (1024*1024)\n",
        "          total_size_mb += size_mb\n",
        "          print(f\"    - {os.path.basename(f)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "      print(f\"  • Validation files: {len(val_files)}\")\n",
        "      for f in val_files:\n",
        "          size_mb = os.path.getsize(f) / (1024*1024)\n",
        "          total_size_mb += size_mb\n",
        "          print(f\"    - {os.path.basename(f)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "      print(f\"  📊 Tamaño total: {total_size_mb:.1f} MB\")\n",
        "\n",
        "      if not train_files:\n",
        "          raise FileNotFoundError(f\"❌ No se encontraron archivos NMT_train*.csv en {dataset_path}\")\n",
        "\n",
        "      # Configurar límites para evitar colapso de memoria\n",
        "      memory_limit_mb = 2000\n",
        "      samples_per_mb = 100\n",
        "      max_samples_per_file = max(1000, int(memory_limit_mb * samples_per_mb / len(train_files)))\n",
        "\n",
        "      print(f\"🧠 Configuración de memoria:\")\n",
        "      print(f\"  • Límite total: {memory_limit_mb} MB\")\n",
        "      print(f\"  • Max samples por archivo: {max_samples_per_file}\")\n",
        "\n",
        "      def load_files_batch(file_list, file_type=\"training\"):\n",
        "          \"\"\"Carga archivos en batches para optimizar memoria\"\"\"\n",
        "          all_pairs = []\n",
        "\n",
        "          print(f\"\\n⚡ Cargando {len(file_list)} archivos de {file_type}...\")\n",
        "\n",
        "          for i, file_path in enumerate(file_list):\n",
        "              print(f\"  📥 [{i+1}/{len(file_list)}] {os.path.basename(file_path)}\")\n",
        "\n",
        "              file_pairs = load_tokenized_csv(file_path, max_samples_per_file)\n",
        "              all_pairs.extend(file_pairs)\n",
        "\n",
        "              del file_pairs\n",
        "              gc.collect()\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  memory_used = torch.cuda.memory_allocated() / 1e9\n",
        "                  print(f\"    💾 GPU Memory: {memory_used:.2f} GB\")\n",
        "\n",
        "          print(f\"  🎯 Total {file_type}: {len(all_pairs):,} pares tokenizados\")\n",
        "          return all_pairs\n",
        "\n",
        "      train_pairs = load_files_batch(train_files, \"entrenamiento\")\n",
        "\n",
        "      if val_files:\n",
        "          val_pairs = load_files_batch(val_files, \"validación\")\n",
        "      else:\n",
        "          print(\"\\n⚠️ No hay archivos de validación, creando subset...\")\n",
        "          split_size = min(2000, max(100, len(train_pairs) // 10))\n",
        "          val_pairs = train_pairs[-split_size:]\n",
        "          train_pairs = train_pairs[:-split_size]\n",
        "          print(f\"  🔀 {len(val_pairs):,} pares movidos a validación\")\n",
        "\n",
        "      return train_pairs, val_pairs\n",
        "\n",
        "  def limit_samples_per_epoch(train_pairs, val_pairs, config):\n",
        "      \"\"\"Limita samples por época para épocas más cortas\"\"\"\n",
        "\n",
        "      print(f\"\\n✂️ LIMITANDO SAMPLES POR ÉPOCA:\")\n",
        "\n",
        "      original_train = len(train_pairs)\n",
        "      original_val = len(val_pairs)\n",
        "\n",
        "      # Limitar training samples\n",
        "      if len(train_pairs) > config.max_samples_per_epoch:\n",
        "          # Usar subset random pero reproducible\n",
        "          random.seed(42)\n",
        "\n",
        "          # Mezclar y tomar subset\n",
        "          train_indices = list(range(len(train_pairs)))\n",
        "          random.shuffle(train_indices)\n",
        "\n",
        "          limited_train = [train_pairs[i] for i in train_indices[:config.max_samples_per_epoch]]\n",
        "      else:\n",
        "          limited_train = train_pairs\n",
        "\n",
        "      # Limitar validation samples (proporcionalmente)\n",
        "      val_ratio = len(val_pairs) / len(train_pairs) if len(train_pairs) > 0 else 0.1\n",
        "      max_val_samples = int(config.max_samples_per_epoch * val_ratio)\n",
        "      max_val_samples = max(100, min(1000, max_val_samples))  # Entre 100 y 1000\n",
        "\n",
        "      if len(val_pairs) > max_val_samples:\n",
        "          random.seed(42)\n",
        "          val_indices = list(range(len(val_pairs)))\n",
        "          random.shuffle(val_indices)\n",
        "          limited_val = [val_pairs[i] for i in val_indices[:max_val_samples]]\n",
        "      else:\n",
        "          limited_val = val_pairs\n",
        "\n",
        "      print(f\"  • Train: {original_train:,} → {len(limited_train):,}\")\n",
        "      print(f\"  • Val: {original_val:,} → {len(limited_val):,}\")\n",
        "\n",
        "      if original_train > 0:\n",
        "          reduction_pct = ((original_train - len(limited_train)) / original_train * 100)\n",
        "          print(f\"  • Reducción: {reduction_pct:.1f}%\")\n",
        "\n",
        "      # Estimar tiempo\n",
        "      batches_per_epoch = len(limited_train) // config.batch_size\n",
        "      estimated_minutes = (batches_per_epoch * 0.2) / 60  # ~0.2 seg por batch\n",
        "      print(f\"  • Batches por época: {batches_per_epoch:,}\")\n",
        "      print(f\"  • Tiempo estimado por época: ~{estimated_minutes:.0f}-{estimated_minutes*1.5:.0f} minutos\")\n",
        "\n",
        "      return limited_train, limited_val\n",
        "\n",
        "  def setup_model_saving(models_path):\n",
        "      \"\"\"Configura sistema de guardado\"\"\"\n",
        "      global drive_models_path\n",
        "      drive_models_path = models_path\n",
        "      print(f\"💾 Sistema de guardado configurado en: {models_path}\")\n",
        "      return models_path\n",
        "\n",
        "  def analyze_tokenized_data(train_pairs, val_pairs):\n",
        "      \"\"\"Analiza las características de los datos tokenizados\"\"\"\n",
        "\n",
        "      if len(train_pairs) == 0:\n",
        "          print(\"❌ No hay datos para analizar\")\n",
        "          return\n",
        "\n",
        "      print(f\"\\n🔍 ANÁLISIS DE DATOS TOKENIZADOS:\")\n",
        "      print(f\"  • Training samples: {len(train_pairs):,}\")\n",
        "      print(f\"  • Validation samples: {len(val_pairs):,}\")\n",
        "\n",
        "      # Analizar longitudes\n",
        "      input_lengths = [len(pair['input_ids']) for pair in train_pairs[:1000]]\n",
        "      target_lengths = [len(pair['labels']) for pair in train_pairs[:1000]]\n",
        "\n",
        "      print(f\"\\n📏 ESTADÍSTICAS DE LONGITUD (muestra de 1000):\")\n",
        "      print(f\"  • Input - Promedio: {sum(input_lengths)/len(input_lengths):.1f}, Max: {max(input_lengths)}\")\n",
        "      print(f\"  • Target - Promedio: {sum(target_lengths)/len(target_lengths):.1f}, Max: {max(target_lengths)}\")\n",
        "\n",
        "      # Analizar idiomas\n",
        "      src_langs = {}\n",
        "      tgt_langs = {}\n",
        "      sample_size = min(1000, len(train_pairs))\n",
        "\n",
        "      for pair in train_pairs[:sample_size]:\n",
        "          src_lang = pair['src_lang']\n",
        "          tgt_lang = pair['tgt_lang']\n",
        "          src_langs[src_lang] = src_langs.get(src_lang, 0) + 1\n",
        "          tgt_langs[tgt_lang] = tgt_langs.get(tgt_lang, 0) + 1\n",
        "\n",
        "      print(f\"\\n🌍 IDIOMAS DETECTADOS (muestra de {sample_size}):\")\n",
        "      print(f\"  • Origen: {dict(sorted(src_langs.items()))}\")\n",
        "      print(f\"  • Destino: {dict(sorted(tgt_langs.items()))}\")\n",
        "\n",
        "      # Mostrar ejemplo\n",
        "      if len(train_pairs) > 0:\n",
        "          example = train_pairs[0]\n",
        "          print(f\"\\n📝 EJEMPLO DE DATOS:\")\n",
        "          print(f\"  • Input tokens: {example['input_ids'][:20].tolist()}... (len: {len(example['input_ids'])})\")\n",
        "          print(f\"  • Target tokens: {example['labels'][:20].tolist()}... (len: {len(example['labels'])})\")\n",
        "          print(f\"  • Idiomas: {example['src_lang']} → {example['tgt_lang']}\")\n",
        "\n",
        "  def prepare_tokenized_training_data():\n",
        "      \"\"\"Función principal para datos pre-tokenizados\"\"\"\n",
        "\n",
        "      try:\n",
        "          print(\"🚀 INICIANDO CARGA DE DATOS PRE-TOKENIZADOS CON ÉPOCAS OPTIMIZADAS\")\n",
        "          print(\"=\" * 80)\n",
        "\n",
        "          # Configurar rutas\n",
        "          dataset_path, models_path = mount_drive_and_setup_paths()\n",
        "          setup_model_saving(models_path)\n",
        "\n",
        "          # Optimizar garbage collection\n",
        "          gc.set_threshold(500, 10, 10)\n",
        "          gc.collect()\n",
        "\n",
        "          # Cargar datos tokenizados\n",
        "          train_pairs, val_pairs = load_fragmented_tokenized_data(dataset_path)\n",
        "\n",
        "          if len(train_pairs) == 0:\n",
        "              raise ValueError(\"No se pudieron cargar datos tokenizados válidos\")\n",
        "\n",
        "          # LIMITAR SAMPLES POR ÉPOCA (NUEVO)\n",
        "          train_pairs, val_pairs = limit_samples_per_epoch(train_pairs, val_pairs, config)\n",
        "\n",
        "          # Analizar datos\n",
        "          analyze_tokenized_data(train_pairs, val_pairs)\n",
        "\n",
        "          print(f\"\\n✅ DATOS TOKENIZADOS LISTOS PARA ENTRENAMIENTO OPTIMIZADO\")\n",
        "          print(f\"📊 Dataset: ✓ | 💾 Guardado: ✓ | ✂️ Épocas cortas: ✓ | 🧹 Memoria optimizada: ✓\")\n",
        "\n",
        "          return train_pairs, val_pairs\n",
        "\n",
        "      except FileNotFoundError as e:\n",
        "          print(f\"\\n📂 {str(e)}\")\n",
        "          print(\"💡 Verifica que tus archivos CSV estén en la ruta correcta\")\n",
        "          return create_example_tokenized_data()\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"\\n❌ ERROR: {str(e)}\")\n",
        "          print(\"💡 Usando datos de ejemplo...\")\n",
        "          return create_example_tokenized_data()\n",
        "\n",
        "  def create_example_tokenized_data():\n",
        "      \"\"\"Crea datos tokenizados de ejemplo para testing\"\"\"\n",
        "      print(\"🔧 Generando datos tokenizados de ejemplo...\")\n",
        "\n",
        "      example_pairs = []\n",
        "\n",
        "      for i in range(5):\n",
        "          input_ids = torch.tensor([119, 104, 118, 119, 35] + [100+i]*10 + [1])\n",
        "          attention_mask = torch.ones_like(input_ids)\n",
        "          labels = torch.tensor([104, 109, 104, 112, 115, 111, 114, 35] + [200+i]*5 + [1])\n",
        "          target_attention_mask = torch.ones_like(labels)\n",
        "\n",
        "          example_pairs.append({\n",
        "              'input_ids': input_ids,\n",
        "              'attention_mask': attention_mask,\n",
        "              'labels': labels,\n",
        "              'target_attention_mask': target_attention_mask,\n",
        "              'src_lang': 'EN' if i % 2 == 0 else 'ES',\n",
        "              'tgt_lang': 'ES' if i % 2 == 0 else 'SW'\n",
        "          })\n",
        "\n",
        "      # Limitar también los ejemplos\n",
        "      limited_train = example_pairs[:4]\n",
        "      limited_val = example_pairs[4:]\n",
        "\n",
        "      # Simular limitación\n",
        "      if hasattr(config, 'max_samples_per_epoch'):\n",
        "          limited_train, limited_val = limit_samples_per_epoch(limited_train, limited_val, config)\n",
        "\n",
        "      print(f\"✅ Datos de ejemplo creados: {len(limited_train)} train, {len(limited_val)} val\")\n",
        "      return limited_train, limited_val\n",
        "\n",
        "  # Variables globales\n",
        "  drive_models_path = None\n",
        "\n",
        "  # EJECUTAR: Cargar datos tokenizados\n",
        "  print(\"Iniciando carga de datos pre-tokenizados con épocas optimizadas...\")\n",
        "  train_pairs, val_pairs = prepare_tokenized_training_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BAKXpdBZ2jO",
        "outputId": "42328b2f-308b-4339-9ae2-eaacaa45c7f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando carga de datos pre-tokenizados con épocas optimizadas...\n",
            "🚀 INICIANDO CARGA DE DATOS PRE-TOKENIZADOS CON ÉPOCAS OPTIMIZADAS\n",
            "================================================================================\n",
            "🔌 Montando Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive montado exitosamente\n",
            "📂 Dataset path: /content/drive/MyDrive/GlobalTranslator/NMT/Dataset/\n",
            "💾 Models path: /content/drive/MyDrive/GlobalTranslator/NMT/Models/\n",
            "💾 Sistema de guardado configurado en: /content/drive/MyDrive/GlobalTranslator/NMT/Models/\n",
            "\n",
            "📁 ARCHIVOS TOKENIZADOS ENCONTRADOS:\n",
            "  • Training files: 2\n",
            "    - NMT_train17.csv (180.9 MB)\n",
            "    - NMT_train18.csv (181.0 MB)\n",
            "  • Validation files: 1\n",
            "    - NMT_val3.csv (180.9 MB)\n",
            "  📊 Tamaño total: 542.8 MB\n",
            "🧠 Configuración de memoria:\n",
            "  • Límite total: 2000 MB\n",
            "  • Max samples por archivo: 100000\n",
            "\n",
            "⚡ Cargando 2 archivos de entrenamiento...\n",
            "  📥 [1/2] NMT_train17.csv\n",
            "📥 Cargando NMT_train17.csv...\n",
            "  📊 Filas encontradas: 100000\n",
            "  ✅ Pares válidos: 100000\n",
            "  📥 [2/2] NMT_train18.csv\n",
            "📥 Cargando NMT_train18.csv...\n",
            "  📊 Filas encontradas: 100000\n",
            "  ✅ Pares válidos: 100000\n",
            "  🎯 Total entrenamiento: 200,000 pares tokenizados\n",
            "\n",
            "⚡ Cargando 1 archivos de validación...\n",
            "  📥 [1/1] NMT_val3.csv\n",
            "📥 Cargando NMT_val3.csv...\n",
            "  📊 Filas encontradas: 100000\n",
            "  ✅ Pares válidos: 100000\n",
            "  🎯 Total validación: 100,000 pares tokenizados\n",
            "\n",
            "✂️ LIMITANDO SAMPLES POR ÉPOCA:\n",
            "  • Train: 200,000 → 50,000\n",
            "  • Val: 100,000 → 1,000\n",
            "  • Reducción: 75.0%\n",
            "  • Batches por época: 6,250\n",
            "  • Tiempo estimado por época: ~21-31 minutos\n",
            "\n",
            "🔍 ANÁLISIS DE DATOS TOKENIZADOS:\n",
            "  • Training samples: 50,000\n",
            "  • Validation samples: 1,000\n",
            "\n",
            "📏 ESTADÍSTICAS DE LONGITUD (muestra de 1000):\n",
            "  • Input - Promedio: 128.0, Max: 128\n",
            "  • Target - Promedio: 128.0, Max: 128\n",
            "\n",
            "🌍 IDIOMAS DETECTADOS (muestra de 1000):\n",
            "  • Origen: {'ES': 659, 'IT': 341}\n",
            "  • Destino: {'IT': 325, 'SW': 675}\n",
            "\n",
            "📝 EJEMPLO DE DATOS:\n",
            "  • Input tokens: [119, 117, 100, 113, 118, 111, 100, 119, 104, 35, 76, 119, 100, 111, 108, 100, 113, 35, 119, 114]... (len: 128)\n",
            "  • Target tokens: [119, 117, 100, 113, 118, 111, 100, 119, 104, 35, 61, 35, 113, 100, 35, 108, 110, 108, 122, 100]... (len: 128)\n",
            "  • Idiomas: IT → SW\n",
            "\n",
            "✅ DATOS TOKENIZADOS LISTOS PARA ENTRENAMIENTO OPTIMIZADO\n",
            "📊 Dataset: ✓ | 💾 Guardado: ✓ | ✂️ Épocas cortas: ✓ | 🧹 Memoria optimizada: ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 7: Función de entrenamiento"
      ],
      "metadata": {
        "id": "sfYjDgB4Z6pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, epoch, config):\n",
        "    \"\"\"Entrena una época\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Mover datos a device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        src_langs = batch['src_langs']\n",
        "        tgt_langs = batch['tgt_langs']\n",
        "\n",
        "        # Determinar si usar NLLB basado en el primer elemento del batch\n",
        "        use_nllb = src_langs[0] in config.nllb_languages\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            src_lang=src_langs[0],\n",
        "            tgt_lang=tgt_langs[0],\n",
        "            use_nllb=use_nllb\n",
        "        )\n",
        "\n",
        "        loss = outputs['loss']\n",
        "\n",
        "        # Gradient accumulation\n",
        "        loss = loss / config.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
        "\n",
        "        # Actualizar progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item() * config.gradient_accumulation_steps,\n",
        "            'lr': scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        # Liberar memoria\n",
        "        del loss, outputs\n",
        "        if batch_idx % 10 == 0:\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, device, config):\n",
        "    \"\"\"Valida el modelo\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            src_langs = batch['src_langs']\n",
        "            tgt_langs = batch['tgt_langs']\n",
        "\n",
        "            use_nllb = src_langs[0] in config.nllb_languages\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                src_lang=src_langs[0],\n",
        "                tgt_lang=tgt_langs[0],\n",
        "                use_nllb=use_nllb\n",
        "            )\n",
        "\n",
        "            total_loss += outputs['loss'].item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "r7HCUjICZ7o8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 8: Loop principal de entrenamiento para datos PRE-TOKENIZADOS"
      ],
      "metadata": {
        "id": "Z7AF89YtZ9np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  def train_epoch_tokenized(model, dataloader, optimizer, scheduler, device, epoch, config):\n",
        "      \"\"\"Entrena una época con datos pre-tokenizados\"\"\"\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "\n",
        "      for batch_idx, batch in enumerate(progress_bar):\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "          src_langs = batch['src_langs']\n",
        "          tgt_langs = batch['tgt_langs']\n",
        "\n",
        "          # MAPEO DE IDIOMAS MEJORADO\n",
        "          src_lang_code = src_langs[0]\n",
        "          tgt_lang_code = tgt_langs[0]\n",
        "\n",
        "          # Usar mapeo de la configuración\n",
        "          lang_mapping = config.language_mapping\n",
        "          mapped_src = lang_mapping.get(src_lang_code, src_lang_code)\n",
        "          mapped_tgt = lang_mapping.get(tgt_lang_code, tgt_lang_code)\n",
        "\n",
        "          # Verificar soporte NLLB usando la configuración\n",
        "          use_nllb = config.is_supported_by_nllb(src_lang_code) and config.is_supported_by_nllb(tgt_lang_code)\n",
        "\n",
        "          # Forward pass\n",
        "          try:\n",
        "              outputs = model(\n",
        "                  input_ids=input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  labels=labels,\n",
        "                  src_lang=mapped_src,\n",
        "                  tgt_lang=mapped_tgt,\n",
        "                  use_nllb=use_nllb\n",
        "              )\n",
        "\n",
        "              loss = outputs['loss']\n",
        "\n",
        "              if loss is None or torch.isnan(loss):\n",
        "                  print(f\"⚠️ Loss is None or NaN at batch {batch_idx}\")\n",
        "                  continue\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"❌ Error en forward pass: {str(e)}\")\n",
        "              print(f\"Input shape: {input_ids.shape}, Labels shape: {labels.shape}\")\n",
        "              continue\n",
        "\n",
        "          # Gradient accumulation\n",
        "          loss = loss / config.gradient_accumulation_steps\n",
        "          loss.backward()\n",
        "\n",
        "          if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "          total_loss += loss.item() * config.gradient_accumulation_steps\n",
        "\n",
        "          # Actualizar progress bar\n",
        "          progress_bar.set_postfix({\n",
        "              'loss': loss.item() * config.gradient_accumulation_steps,\n",
        "              'lr': scheduler.get_last_lr()[0],\n",
        "              'use_nllb': use_nllb,\n",
        "              'epoch': f\"{epoch+1}/{config.num_epochs}\"\n",
        "          })\n",
        "\n",
        "          # Liberar memoria periódicamente\n",
        "          del loss, outputs\n",
        "          if batch_idx % 20 == 0:\n",
        "              torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "      return total_loss / len(dataloader)\n",
        "\n",
        "  def validate_tokenized(model, dataloader, device, config):\n",
        "      \"\"\"Valida el modelo con datos pre-tokenizados\"\"\"\n",
        "      model.eval()\n",
        "      total_loss = 0\n",
        "      num_batches = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "              input_ids = batch['input_ids'].to(device)\n",
        "              attention_mask = batch['attention_mask'].to(device)\n",
        "              labels = batch['labels'].to(device)\n",
        "              src_langs = batch['src_langs']\n",
        "              tgt_langs = batch['tgt_langs']\n",
        "\n",
        "              # Mapear idiomas usando configuración\n",
        "              src_lang_code = src_langs[0]\n",
        "              tgt_lang_code = tgt_langs[0]\n",
        "              lang_mapping = config.language_mapping\n",
        "              mapped_src = lang_mapping.get(src_lang_code, src_lang_code)\n",
        "              mapped_tgt = lang_mapping.get(tgt_lang_code, tgt_lang_code)\n",
        "\n",
        "              use_nllb = config.is_supported_by_nllb(src_lang_code) and config.is_supported_by_nllb(tgt_lang_code)\n",
        "\n",
        "              try:\n",
        "                  outputs = model(\n",
        "                      input_ids=input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      src_lang=mapped_src,\n",
        "                      tgt_lang=mapped_tgt,\n",
        "                      use_nllb=use_nllb\n",
        "                  )\n",
        "\n",
        "                  if outputs['loss'] is not None:\n",
        "                      total_loss += outputs['loss'].item()\n",
        "                      num_batches += 1\n",
        "\n",
        "              except Exception as e:\n",
        "                  print(f\"⚠️ Error en validación: {str(e)}\")\n",
        "                  continue\n",
        "\n",
        "      return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "  def train_hybrid_model_tokenized(model, train_pairs, val_pairs, config, device):\n",
        "      \"\"\"Entrena el modelo híbrido con datos PRE-TOKENIZADOS y épocas optimizadas\"\"\"\n",
        "\n",
        "      print(f\"🏗️ CONFIGURANDO ENTRENAMIENTO OPTIMIZADO PARA ÉPOCAS CORTAS\")\n",
        "      print(f\"=\" * 70)\n",
        "\n",
        "      # Crear dataloaders para datos tokenizados\n",
        "      print(\"📦 Creando dataloaders...\")\n",
        "      train_loader, val_loader = create_tokenized_dataloaders(\n",
        "          train_pairs, val_pairs,\n",
        "          batch_size=config.batch_size,\n",
        "          max_length=config.max_length\n",
        "      )\n",
        "\n",
        "      # Verificar un batch de ejemplo\n",
        "      print(\"\\n🔍 Verificando formato de datos...\")\n",
        "      sample_batch = next(iter(train_loader))\n",
        "      print(f\"  • Input shape: {sample_batch['input_ids'].shape}\")\n",
        "      print(f\"  • Labels shape: {sample_batch['labels'].shape}\")\n",
        "      print(f\"  • Idiomas ejemplo: {sample_batch['src_langs'][0]} → {sample_batch['tgt_langs'][0]}\")\n",
        "      print(f\"  • Tokens ejemplo: {sample_batch['input_ids'][0][:10].tolist()}...\")\n",
        "\n",
        "      # Configurar optimizador\n",
        "      print(\"\\n⚙️ Configurando optimizador...\")\n",
        "      optimizer = torch.optim.AdamW(\n",
        "          model.parameters(),\n",
        "          lr=config.learning_rate,\n",
        "          weight_decay=0.01,\n",
        "          eps=1e-8\n",
        "      )\n",
        "\n",
        "      # Configurar scheduler\n",
        "      total_steps = len(train_loader) * config.num_epochs\n",
        "\n",
        "      if hasattr(config, 'lr_scheduler_type') and config.lr_scheduler_type == \"cosine\":\n",
        "          from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "          scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=config.min_lr)\n",
        "      else:\n",
        "          scheduler = get_linear_schedule_with_warmup(\n",
        "              optimizer,\n",
        "              num_warmup_steps=config.warmup_steps,\n",
        "              num_training_steps=total_steps\n",
        "          )\n",
        "\n",
        "      print(f\"  • Learning rate: {config.learning_rate}\")\n",
        "      print(f\"  • Total steps: {total_steps}\")\n",
        "      print(f\"  • Scheduler: {getattr(config, 'lr_scheduler_type', 'linear')}\")\n",
        "\n",
        "      # CONFIGURACIÓN DE ENTRENAMIENTO OPTIMIZADO\n",
        "      history = {'train_loss': [], 'val_loss': []}\n",
        "      best_val_loss = float('inf')\n",
        "      patience_counter = 0\n",
        "      epoch_losses = []\n",
        "\n",
        "      print(f\"\\n💾 CONFIGURACIÓN DE GUARDADO Y EARLY STOPPING:\")\n",
        "      print(f\"  • Guardar cada: {config.save_every_n_epochs} época(s)\")\n",
        "      print(f\"  • Early stopping patience: {config.early_stopping_patience} épocas\")\n",
        "      print(f\"  • Samples por época: {len(train_pairs):,}\")\n",
        "      print(f\"  • Batches por época: {len(train_loader):,}\")\n",
        "\n",
        "      print(f\"\\n🚀 INICIANDO ENTRENAMIENTO OPTIMIZADO...\")\n",
        "      print(f\"  • Épocas: {config.num_epochs}\")\n",
        "      print(f\"  • Batch size: {config.batch_size}\")\n",
        "      print(f\"  • Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
        "\n",
        "      # Training loop optimizado\n",
        "      for epoch in range(config.num_epochs):\n",
        "          print(f\"\\n{'='*70}\")\n",
        "          print(f\"ÉPOCA {epoch+1}/{config.num_epochs}\")\n",
        "          print(f\"{'='*70}\")\n",
        "\n",
        "          # Entrenar\n",
        "          try:\n",
        "              train_loss = train_epoch_tokenized(\n",
        "                  model, train_loader, optimizer, scheduler, device, epoch, config\n",
        "              )\n",
        "              history['train_loss'].append(train_loss)\n",
        "\n",
        "              print(f\"\\n📊 Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"❌ Error en entrenamiento: {e}\")\n",
        "              print(\"Intentando continuar...\")\n",
        "              continue\n",
        "\n",
        "          # Validar\n",
        "          try:\n",
        "              val_loss = validate_tokenized(model, val_loader, device, config)\n",
        "              history['val_loss'].append(val_loss)\n",
        "\n",
        "              print(f\"📊 Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"❌ Error en validación: {e}\")\n",
        "              val_loss = float('inf')\n",
        "              history['val_loss'].append(val_loss)\n",
        "\n",
        "          # GUARDADO CADA ÉPOCA (NUEVO)\n",
        "          if (epoch + 1) % config.save_every_n_epochs == 0:\n",
        "              checkpoint_name = f\"checkpoint_epoch_{epoch+1}.pt\"\n",
        "              checkpoint_data = {\n",
        "                  'epoch': epoch,\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'scheduler_state_dict': scheduler.state_dict(),\n",
        "                  'train_loss': train_loss,\n",
        "                  'val_loss': val_loss,\n",
        "                  'config': config.__dict__,\n",
        "                  'history': history,\n",
        "                  'epoch_losses': epoch_losses\n",
        "              }\n",
        "              torch.save(checkpoint_data, checkpoint_name)\n",
        "              print(f\"💾 Checkpoint guardado: {checkpoint_name}\")\n",
        "\n",
        "          # Guardar mejor modelo\n",
        "          if val_loss < best_val_loss:\n",
        "              best_val_loss = val_loss\n",
        "              patience_counter = 0\n",
        "\n",
        "              # Guardar como mejor modelo\n",
        "              best_checkpoint_data = {\n",
        "                  'epoch': epoch,\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'train_loss': train_loss,\n",
        "                  'val_loss': val_loss,\n",
        "                  'config': config.__dict__\n",
        "              }\n",
        "\n",
        "              torch.save(best_checkpoint_data, 'best_hybrid_model_tokenized.pt')\n",
        "              print(\"🏆 ¡NUEVO MEJOR MODELO GUARDADO!\")\n",
        "\n",
        "          else:\n",
        "              patience_counter += 1\n",
        "              print(f\"⏳ Paciencia: {patience_counter}/{config.early_stopping_patience}\")\n",
        "\n",
        "          # EARLY STOPPING (NUEVO)\n",
        "          if patience_counter >= config.early_stopping_patience:\n",
        "              print(f\"\\n🛑 EARLY STOPPING ACTIVADO!\")\n",
        "              print(f\"   • {config.early_stopping_patience} épocas consecutivas sin mejora\")\n",
        "              print(f\"   • Mejor val_loss: {best_val_loss:.4f} (época {epoch+1-patience_counter})\")\n",
        "              print(f\"   • Entrenamiento finalizado en época {epoch+1}\")\n",
        "              break\n",
        "\n",
        "          # Guardar estadísticas de la época\n",
        "          epoch_losses.append({\n",
        "              'epoch': epoch+1,\n",
        "              'train_loss': train_loss,\n",
        "              'val_loss': val_loss,\n",
        "              'lr': scheduler.get_last_lr()[0],\n",
        "              'patience': patience_counter\n",
        "          })\n",
        "\n",
        "          # Mostrar resumen de época\n",
        "          print(f\"\\n📈 RESUMEN ÉPOCA {epoch+1}:\")\n",
        "          print(f\"  • Train Loss: {train_loss:.4f}\")\n",
        "          print(f\"  • Val Loss: {val_loss:.4f}\")\n",
        "          print(f\"  • Best Val Loss: {best_val_loss:.4f}\")\n",
        "          print(f\"  • Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "          print(f\"  • Paciencia: {patience_counter}/{config.early_stopping_patience}\")\n",
        "\n",
        "          # Estimación de tiempo restante\n",
        "          remaining_epochs = config.num_epochs - (epoch + 1)\n",
        "          estimated_time = remaining_epochs * 50  # ~50 min por época\n",
        "          print(f\"  • Épocas restantes: {remaining_epochs}\")\n",
        "          print(f\"  • Tiempo estimado restante: ~{estimated_time} minutos\")\n",
        "\n",
        "          # Limpiar memoria\n",
        "          torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "          gc.collect()\n",
        "\n",
        "      print(f\"\\n🎉 ENTRENAMIENTO COMPLETADO!\")\n",
        "      print(f\"  • Mejor Val Loss: {best_val_loss:.4f}\")\n",
        "      print(f\"  • Épocas completadas: {len(history['train_loss'])}\")\n",
        "      print(f\"  • Modelo guardado en: best_hybrid_model_tokenized.pt\")\n",
        "\n",
        "      # Guardar historial final\n",
        "      final_history = {\n",
        "          'history': history,\n",
        "          'epoch_details': epoch_losses,\n",
        "          'best_val_loss': best_val_loss,\n",
        "          'config': config.__dict__\n",
        "      }\n",
        "      torch.save(final_history, 'training_history.pt')\n",
        "      print(f\"  • Historial guardado en: training_history.pt\")\n",
        "\n",
        "      return history\n",
        "\n",
        "  # Función wrapper para compatibilidad con el código existente\n",
        "  def train_hybrid_model(model, train_pairs, val_pairs, config, device):\n",
        "      \"\"\"Función wrapper que detecta el tipo de datos y usa la función apropiada\"\"\"\n",
        "\n",
        "      if len(train_pairs) > 0:\n",
        "          if isinstance(train_pairs[0], dict) and 'input_ids' in train_pairs[0]:\n",
        "              print(\"🔍 Detectados datos PRE-TOKENIZADOS\")\n",
        "              return train_hybrid_model_tokenized(model, train_pairs, val_pairs, config, device)\n",
        "          else:\n",
        "              print(\"🔍 Detectados datos de TEXTO RAW\")\n",
        "              raise NotImplementedError(\"Función para texto raw no implementada en esta versión\")\n",
        "      else:\n",
        "          raise ValueError(\"No hay datos de entrenamiento\")\n"
      ],
      "metadata": {
        "id": "93kf3NRMZ-ge"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 9: Inicialización y entrenamiento"
      ],
      "metadata": {
        "id": "ddo_v4qyaASB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo\n",
        "print(\"Initializing Hybrid Model...\")\n",
        "model = HybridNLLBByT5Model(config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Contar parámetros\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Entrenar modelo\n",
        "print(\"\\nStarting training...\")\n",
        "history = train_hybrid_model(model, train_pairs, val_pairs, config, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875,
          "referenced_widgets": [
            "85216c0b7d4c495c99b61b8c5a822fc6",
            "0e36fcc0bbaf43a69659938b29c5add9",
            "bddece4f8df944d5954516d21841d840",
            "627829a0e4934cd9ad862c3dab9a0a66",
            "cbbec8a339f94399b271100d3032e8a9",
            "4ec1f9314df84d82b1259c59165e3d59",
            "ff25cb3c37c04f089178674ab95aebe4",
            "cc3d2e0ffdbb4903b888ce2ed84bc38e",
            "34949c49330b4c58a346e572deb91211",
            "68d6ddc4b06c45298e4dbd8f2c64266a",
            "3614f9036c9442c298c6eff0c1c53042"
          ]
        },
        "id": "xEdhKib5aBPS",
        "outputId": "92f2cd6c-2cad-4ec0-fcd9-dcc2847b1199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Hybrid Model...\n",
            "Loading NLLB model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ByT5 model...\n",
            "NLLB hidden size: 1024\n",
            "ByT5 hidden size: 1472\n",
            "ByT5 vocab size: 384\n",
            "Total parameters: 921,689,280\n",
            "Trainable parameters: 921,689,280\n",
            "\n",
            "Starting training...\n",
            "🔍 Detectados datos PRE-TOKENIZADOS\n",
            "🏗️ CONFIGURANDO ENTRENAMIENTO OPTIMIZADO PARA ÉPOCAS CORTAS\n",
            "======================================================================\n",
            "📦 Creando dataloaders...\n",
            "✅ Dataloaders creados:\n",
            "  • Train batches: 6250\n",
            "  • Val batches: 125\n",
            "  • Batch size: 8\n",
            "  • Max length: 64\n",
            "\n",
            "🔍 Verificando formato de datos...\n",
            "  • Input shape: torch.Size([8, 64])\n",
            "  • Labels shape: torch.Size([8, 64])\n",
            "  • Idiomas ejemplo: ES → SW\n",
            "  • Tokens ejemplo: [119, 117, 100, 113, 118, 111, 100, 119, 104, 35]...\n",
            "\n",
            "⚙️ Configurando optimizador...\n",
            "  • Learning rate: 1e-05\n",
            "  • Total steps: 62500\n",
            "  • Scheduler: cosine\n",
            "\n",
            "💾 CONFIGURACIÓN DE GUARDADO Y EARLY STOPPING:\n",
            "  • Guardar cada: 1 época(s)\n",
            "  • Early stopping patience: 3 épocas\n",
            "  • Samples por época: 50,000\n",
            "  • Batches por época: 6,250\n",
            "\n",
            "🚀 INICIANDO ENTRENAMIENTO OPTIMIZADO...\n",
            "  • Épocas: 10\n",
            "  • Batch size: 8\n",
            "  • Gradient accumulation: 4\n",
            "\n",
            "======================================================================\n",
            "ÉPOCA 1/10\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/6250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85216c0b7d4c495c99b61b8c5a822fc6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 10: Función de inferencia y pruebas"
      ],
      "metadata": {
        "id": "5UtvUGsAaCpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_translation(model, test_pairs, device):\n",
        "    \"\"\"Prueba el modelo con pares de prueba\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for source_text, expected_target, src_lang, tgt_lang in test_pairs:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Source ({src_lang}): {source_text}\")\n",
        "        print(f\"Expected ({tgt_lang}): {expected_target}\")\n",
        "\n",
        "        # Generar traducción\n",
        "        translation = model.generate_translation(\n",
        "            source_text, src_lang, tgt_lang\n",
        "        )\n",
        "\n",
        "        print(f\"Generated: {translation}\")\n",
        "\n",
        "        results.append({\n",
        "            'source': source_text,\n",
        "            'expected': expected_target,\n",
        "            'generated': translation,\n",
        "            'src_lang': src_lang,\n",
        "            'tgt_lang': tgt_lang\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Pruebas\n",
        "test_pairs = [\n",
        "    (\"Hello, how are you today?\", \"Hola, ¿cómo estás hoy?\", \"eng_Latn\", \"spa_Latn\"),\n",
        "    (\"The weather is nice\", \"El clima está agradable\", \"eng_Latn\", \"spa_Latn\"),\n",
        "    (\"I need help\", \"Necesito ayuda\", \"eng_Latn\", \"spa_Latn\"),\n",
        "]\n",
        "\n",
        "print(\"Testing model...\")\n",
        "results = test_translation(model, test_pairs, device)"
      ],
      "metadata": {
        "id": "G0uJ7sBnaDvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 11: Guardar y cargar modelo"
      ],
      "metadata": {
        "id": "JBP64PeeaFQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_hybrid_model(model, filepath='hybrid_translation_model'):\n",
        "    \"\"\"Guarda el modelo completo\"\"\"\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "    # Guardar configuración\n",
        "    with open(f'{filepath}/config.json', 'w') as f:\n",
        "        json.dump({\n",
        "            'nllb_model_name': model.config.nllb_model_name,\n",
        "            'byt5_model_name': model.config.byt5_model_name,\n",
        "            'hidden_size': model.config.hidden_size,\n",
        "            'fusion_dropout': model.config.fusion_dropout,\n",
        "            'nllb_languages': model.config.nllb_languages,\n",
        "            'new_languages': model.config.new_languages\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Guardar pesos del modelo\n",
        "    torch.save(model.state_dict(), f'{filepath}/model_weights.pt')\n",
        "\n",
        "    print(f\"Model saved to {filepath}\")\n",
        "\n",
        "def load_hybrid_model(filepath='hybrid_translation_model', device='cuda'):\n",
        "    \"\"\"Carga el modelo guardado\"\"\"\n",
        "    # Cargar configuración\n",
        "    with open(f'{filepath}/config.json', 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "\n",
        "    # Recrear configuración\n",
        "    config = HybridTranslationConfig()\n",
        "    for key, value in config_dict.items():\n",
        "        setattr(config, key, value)\n",
        "\n",
        "    # Recrear modelo\n",
        "    model = HybridNLLBByT5Model(config)\n",
        "\n",
        "    # Cargar pesos\n",
        "    model.load_state_dict(torch.load(f'{filepath}/model_weights.pt', map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded from {filepath}\")\n",
        "    return model\n",
        "\n",
        "# Guardar modelo\n",
        "save_hybrid_model(model, 'my_hybrid_translator')"
      ],
      "metadata": {
        "id": "KfRgi4NPaGN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 12: Agregar soporte para nuevos idiomas"
      ],
      "metadata": {
        "id": "STRPlqClaHgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_new_language_support(model, new_language_pairs, config):\n",
        "    \"\"\"\n",
        "    Agrega soporte para un nuevo idioma entrenando solo ByT5\n",
        "    new_language_pairs: Lista de (source, target, src_lang, tgt_lang)\n",
        "    \"\"\"\n",
        "    # Congelar NLLB para preservar conocimiento\n",
        "    for param in model.nllb_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Solo entrenar ByT5 y capas de fusión\n",
        "    trainable_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'nllb' not in name:\n",
        "            param.requires_grad = True\n",
        "            trainable_params.append(param)\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Configurar optimizer solo para parámetros entrenables\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=config.learning_rate * 0.5)\n",
        "\n",
        "    # Crear dataset\n",
        "    tokenizer = model.byt5_tokenizer\n",
        "    dataset = MultilingualTranslationDataset(new_language_pairs, tokenizer, config.max_length)\n",
        "    collator = create_data_collator(tokenizer)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator\n",
        "    )\n",
        "\n",
        "    # Fine-tuning rápido\n",
        "    model.train()\n",
        "    for epoch in range(2):  # Menos épocas para fine-tuning\n",
        "        print(f\"\\nFine-tuning for new language - Epoch {epoch+1}\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                src_lang=batch['src_langs'][0],\n",
        "                tgt_lang=batch['tgt_langs'][0],\n",
        "                use_nllb=False  # Forzar uso de ByT5\n",
        "            )\n",
        "\n",
        "            loss = outputs['loss']\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Descongelar todo para uso normal\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(\"✓ New language support added successfully!\")\n",
        "    return model\n",
        "\n",
        "# Ejemplo de uso para agregar un nuevo idioma\n",
        "new_lang_pairs = [\n",
        "    (\"Hello\", \"Saluton\", \"eng\", \"esperanto\"),\n",
        "    (\"Goodbye\", \"Ĝis revido\", \"eng\", \"esperanto\"),\n",
        "    # Agregar más pares de entrenamiento\n",
        "]\n",
        "\n",
        "# model = add_new_language_support(model, new_lang_pairs, config)"
      ],
      "metadata": {
        "id": "VnTtre-4aIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 13: Evaluación con métricas BLEU"
      ],
      "metadata": {
        "id": "_UXUHzz8aLzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "def evaluate_model_bleu(model, test_pairs, device):\n",
        "    \"\"\"Evalúa el modelo usando BLEU score\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(\"Generating translations for evaluation...\")\n",
        "    for source, target, src_lang, tgt_lang in tqdm(test_pairs):\n",
        "        # Generar traducción\n",
        "        translation = model.generate_translation(\n",
        "            source, src_lang, tgt_lang\n",
        "        )\n",
        "\n",
        "        predictions.append(translation)\n",
        "        references.append([target])  # BLEU espera lista de referencias\n",
        "\n",
        "    # Calcular BLEU score\n",
        "    bleu = corpus_bleu(predictions, references)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"BLEU Score: {bleu.score:.2f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    return bleu.score\n",
        "\n",
        "# Evaluar modelo\n",
        "# bleu_score = evaluate_model_bleu(model, test_pairs, device)"
      ],
      "metadata": {
        "id": "r-otWjZiaMva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 14: Interfaz interactiva para traducción"
      ],
      "metadata": {
        "id": "P30fIlXdaNw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_translation(model, device):\n",
        "    \"\"\"Interfaz interactiva para probar traducciones\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Interactive Translation Interface\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nSupported NLLB languages:\", \", \".join(model.config.nllb_languages[:10]), \"...\")\n",
        "    print(\"\\nType 'quit' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Input source text\n",
        "        source_text = input(\"Enter text to translate (or 'quit'): \").strip()\n",
        "        if source_text.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Input source language\n",
        "        src_lang = input(\"Source language code (e.g., 'eng_Latn'): \").strip()\n",
        "\n",
        "        # Input target language\n",
        "        tgt_lang = input(\"Target language code (e.g., 'spa_Latn'): \").strip()\n",
        "\n",
        "        try:\n",
        "            # Generate translation\n",
        "            print(\"\\nTranslating...\")\n",
        "            translation = model.generate_translation(\n",
        "                source_text, src_lang, tgt_lang\n",
        "            )\n",
        "\n",
        "            print(f\"\\n{'='*40}\")\n",
        "            print(f\"Source ({src_lang}): {source_text}\")\n",
        "            print(f\"Translation ({tgt_lang}): {translation}\")\n",
        "            print(f\"{'='*40}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n\")\n",
        "\n",
        "print(\"\\nThank you for using the translator!\")\n",
        "\n",
        "# Ejecutar interfaz interactiva\n",
        "# interactive_translation(model, device)"
      ],
      "metadata": {
        "id": "z8egz6PJkQY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 15: Preparación de datos desde archivos"
      ],
      "metadata": {
        "id": "vyR4IUo_kUGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_file(filepath, file_format='tsv'):\n",
        "    \"\"\"\n",
        "    Carga pares de traducción desde un archivo\n",
        "    Formato esperado: source_text\\ttarget_text\\tsrc_lang\\ttgt_lang\n",
        "    \"\"\"\n",
        "    data_pairs = []\n",
        "\n",
        "    if file_format == 'tsv':\n",
        "        df = pd.read_csv(filepath, sep='\\t', header=None,\n",
        "                        names=['source', 'target', 'src_lang', 'tgt_lang'])\n",
        "    elif file_format == 'csv':\n",
        "        df = pd.read_csv(filepath)\n",
        "    elif file_format == 'json':\n",
        "        df = pd.read_json(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        data_pairs.append((\n",
        "            row['source'],\n",
        "            row['target'],\n",
        "            row['src_lang'],\n",
        "            row['tgt_lang']\n",
        "        ))\n",
        "\n",
        "    return data_pairs\n",
        "\n",
        "def prepare_parallel_corpus(source_file, target_file, src_lang, tgt_lang):\n",
        "    \"\"\"\n",
        "    Prepara corpus paralelo desde archivos separados\n",
        "    \"\"\"\n",
        "    with open(source_file, 'r', encoding='utf-8') as f:\n",
        "        source_lines = f.readlines()\n",
        "\n",
        "    with open(target_file, 'r', encoding='utf-8') as f:\n",
        "        target_lines = f.readlines()\n",
        "\n",
        "    assert len(source_lines) == len(target_lines), \"Files must have same number of lines\"\n",
        "\n",
        "    data_pairs = []\n",
        "    for src, tgt in zip(source_lines, target_lines):\n",
        "        src = src.strip()\n",
        "        tgt = tgt.strip()\n",
        "        if src and tgt:  # Skip empty lines\n",
        "            data_pairs.append((src, tgt, src_lang, tgt_lang))\n",
        "\n",
        "    return data_pairs\n",
        "\n",
        "# Ejemplo de uso\n",
        "# train_pairs = load_data_from_file('path/to/training_data.tsv')\n",
        "# val_pairs = load_data_from_file('path/to/validation_data.tsv')"
      ],
      "metadata": {
        "id": "nQpTX3ljkUg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 16: Data augmentation para mejorar el entrenamiento"
      ],
      "metadata": {
        "id": "HlKmTairkXA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class DataAugmentation:\n",
        "    \"\"\"Técnicas de augmentation para datos de traducción\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def back_translation(text, model, src_lang, tgt_lang, intermediate_lang='eng_Latn'):\n",
        "        \"\"\"Traducción ida y vuelta para generar variaciones\"\"\"\n",
        "        # Traducir a idioma intermedio\n",
        "        intermediate = model.generate_translation(text, src_lang, intermediate_lang)\n",
        "        # Traducir de vuelta\n",
        "        back_translated = model.generate_translation(intermediate, intermediate_lang, src_lang)\n",
        "        return back_translated\n",
        "\n",
        "    @staticmethod\n",
        "    def token_dropout(text, dropout_prob=0.1):\n",
        "        \"\"\"Elimina tokens aleatoriamente\"\"\"\n",
        "        tokens = text.split()\n",
        "        kept_tokens = [token for token in tokens if random.random() > dropout_prob]\n",
        "        return ' '.join(kept_tokens) if kept_tokens else text\n",
        "\n",
        "    @staticmethod\n",
        "    def token_shuffle(text, shuffle_distance=3):\n",
        "        \"\"\"Mezcla tokens dentro de una ventana\"\"\"\n",
        "        tokens = text.split()\n",
        "        for i in range(len(tokens)):\n",
        "            j = min(len(tokens) - 1, i + random.randint(0, shuffle_distance))\n",
        "            if i != j:\n",
        "                tokens[i], tokens[j] = tokens[j], tokens[i]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    @staticmethod\n",
        "    def synonym_replacement(text, replacement_prob=0.1):\n",
        "        \"\"\"Reemplaza palabras con sinónimos (simplificado)\"\"\"\n",
        "        # Diccionario simple de sinónimos\n",
        "        synonyms = {\n",
        "            'good': ['great', 'excellent', 'nice'],\n",
        "            'bad': ['poor', 'terrible', 'awful'],\n",
        "            'big': ['large', 'huge', 'enormous'],\n",
        "            'small': ['tiny', 'little', 'minute'],\n",
        "            # Agregar más sinónimos según necesidad\n",
        "        }\n",
        "\n",
        "        tokens = text.split()\n",
        "        augmented_tokens = []\n",
        "\n",
        "        for token in tokens:\n",
        "            token_lower = token.lower()\n",
        "            if token_lower in synonyms and random.random() < replacement_prob:\n",
        "                augmented_tokens.append(random.choice(synonyms[token_lower]))\n",
        "            else:\n",
        "                augmented_tokens.append(token)\n",
        "\n",
        "        return ' '.join(augmented_tokens)\n",
        "\n",
        "def augment_training_data(data_pairs, augmentation_factor=2):\n",
        "    \"\"\"Aumenta los datos de entrenamiento\"\"\"\n",
        "    augmenter = DataAugmentation()\n",
        "    augmented_pairs = []\n",
        "\n",
        "    for source, target, src_lang, tgt_lang in data_pairs:\n",
        "        # Agregar par original\n",
        "        augmented_pairs.append((source, target, src_lang, tgt_lang))\n",
        "\n",
        "        # Agregar variaciones\n",
        "        for _ in range(augmentation_factor - 1):\n",
        "            aug_method = random.choice([\n",
        "                augmenter.token_dropout,\n",
        "                augmenter.token_shuffle,\n",
        "                augmenter.synonym_replacement\n",
        "            ])\n",
        "\n",
        "            aug_source = aug_method(source)\n",
        "            augmented_pairs.append((aug_source, target, src_lang, tgt_lang))\n",
        "\n",
        "    random.shuffle(augmented_pairs)\n",
        "    return augmented_pairs\n",
        "\n",
        "# Aumentar datos de entrenamiento\n",
        "# augmented_train_pairs = augment_training_data(train_pairs, augmentation_factor=2)\n",
        "# print(f\"Original pairs: {len(train_pairs)}\")\n",
        "# print(f\"Augmented pairs: {len(augmented_train_pairs)}\")"
      ],
      "metadata": {
        "id": "C67DeatkkYnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 17: Optimización de memoria y velocidad"
      ],
      "metadata": {
        "id": "4_N9lelSkbVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedHybridModel(HybridNLLBByT5Model):\n",
        "    \"\"\"Versión optimizada del modelo híbrido con mixed precision y gradient checkpointing\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # Habilitar gradient checkpointing para ahorrar memoria\n",
        "        if hasattr(self.nllb_model, 'gradient_checkpointing_enable'):\n",
        "            self.nllb_model.gradient_checkpointing_enable()\n",
        "        if hasattr(self.byt5_model, 'gradient_checkpointing_enable'):\n",
        "            self.byt5_model.gradient_checkpointing_enable()\n",
        "\n",
        "        # Configurar mixed precision\n",
        "        self.use_amp = torch.cuda.is_available()\n",
        "\n",
        "    def forward_with_amp(self, *args, **kwargs):\n",
        "        \"\"\"Forward pass con automatic mixed precision\"\"\"\n",
        "        if self.use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                return self.forward(*args, **kwargs)\n",
        "        else:\n",
        "            return self.forward(*args, **kwargs)\n",
        "\n",
        "def train_with_optimization(model, train_pairs, val_pairs, config, device):\n",
        "    \"\"\"Entrenamiento optimizado con mixed precision y gradient accumulation\"\"\"\n",
        "\n",
        "    # Preparar datos\n",
        "    tokenizer = model.byt5_tokenizer\n",
        "    train_dataset = MultilingualTranslationDataset(train_pairs, tokenizer, config.max_length)\n",
        "    val_dataset = MultilingualTranslationDataset(val_pairs, tokenizer, config.max_length)\n",
        "\n",
        "    collator = create_data_collator(tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size * 2,  # Batch más grande para validación\n",
        "        shuffle=False,\n",
        "        collate_fn=collator\n",
        "    )\n",
        "\n",
        "    # Optimizador y scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    total_steps = len(train_loader) * config.num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=config.warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # GradScaler para mixed precision\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Training loop optimizado\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler is not None:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels,\n",
        "                        src_lang=batch['src_langs'][0],\n",
        "                        tgt_lang=batch['tgt_langs'][0]\n",
        "                    )\n",
        "                    loss = outputs['loss'] / config.gradient_accumulation_steps\n",
        "\n",
        "                # Backward pass con scaler\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            else:\n",
        "                # Sin mixed precision\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels,\n",
        "                    src_lang=batch['src_langs'][0],\n",
        "                    tgt_lang=batch['tgt_langs'][0]\n",
        "                )\n",
        "                loss = outputs['loss'] / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item() * config.gradient_accumulation_steps\n",
        "\n",
        "            # Actualizar progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': loss.item() * config.gradient_accumulation_steps,\n",
        "                'lr': scheduler.get_last_lr()[0]\n",
        "            })\n",
        "\n",
        "            # Limpiar cache periódicamente\n",
        "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        val_loss = validate(model, val_loader, device, config)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Guardar mejor modelo\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_optimized_model.pt')\n",
        "            print(\"✓ Saved best model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Usar modelo optimizado\n",
        "# optimized_config = HybridTranslationConfig()\n",
        "# optimized_config.batch_size = 8  # Podemos usar batch más grande con optimizaciones\n",
        "# optimized_model = OptimizedHybridModel(optimized_config).to(device)\n",
        "# optimized_model = train_with_optimization(optimized_model, train_pairs, val_pairs, optimized_config, device)"
      ],
      "metadata": {
        "id": "PRiF66nokcot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 18: Exportar modelo para producción"
      ],
      "metadata": {
        "id": "_bi0zP6mkg0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model_for_production(model, export_path='production_model'):\n",
        "    \"\"\"Exporta el modelo para uso en producción\"\"\"\n",
        "\n",
        "    os.makedirs(export_path, exist_ok=True)\n",
        "\n",
        "    # 1. Guardar el modelo completo en formato PyTorch\n",
        "    torch.save(model, f'{export_path}/complete_model.pt')\n",
        "\n",
        "    # 2. Guardar solo los pesos (más eficiente)\n",
        "    torch.save(model.state_dict(), f'{export_path}/model_weights.pt')\n",
        "\n",
        "    # 3. Exportar a ONNX para interoperabilidad\n",
        "    dummy_input = torch.randint(0, 256, (1, 128)).to(device)\n",
        "    dummy_attention = torch.ones(1, 128).to(device)\n",
        "\n",
        "    try:\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            (dummy_input, dummy_attention),\n",
        "            f'{export_path}/model.onnx',\n",
        "            input_names=['input_ids', 'attention_mask'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={\n",
        "                'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "                'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
        "                'output': {0: 'batch_size', 1: 'sequence'}\n",
        "            }\n",
        "        )\n",
        "        print(\"✓ ONNX export successful\")\n",
        "    except Exception as e:\n",
        "        print(f\"ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Guardar configuración\n",
        "    config_dict = {\n",
        "        'model_type': 'HybridNLLBByT5',\n",
        "        'nllb_model': model.config.nllb_model_name,\n",
        "        'byt5_model': model.config.byt5_model_name,\n",
        "        'supported_languages': {\n",
        "            'nllb': model.config.nllb_languages,\n",
        "            'custom': model.config.new_languages\n",
        "        },\n",
        "        'max_length': model.config.max_length,\n",
        "        'device_requirements': 'CUDA recommended, CPU supported'\n",
        "    }\n",
        "\n",
        "    with open(f'{export_path}/config.json', 'w') as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "\n",
        "    # 5. Crear script de inferencia\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def load_production_model(model_path):\n",
        "    \"\"\"Carga el modelo para producción\"\"\"\n",
        "    model = torch.load(f'{model_path}/complete_model.pt', map_location='cpu')\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def translate(text, src_lang, tgt_lang, model, max_length=256):\n",
        "    \"\"\"Función de traducción simplificada\"\"\"\n",
        "    with torch.no_grad():\n",
        "        translation = model.generate_translation(text, src_lang, tgt_lang, max_length)\n",
        "    return translation\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    model = load_production_model(\".\")\n",
        "    result = translate(\"Hello world\", \"eng_Latn\", \"spa_Latn\", model)\n",
        "    print(result)\n",
        "'''\n",
        "\n",
        "    with open(f'{export_path}/inference.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "\n",
        "    print(f\"\\n✅ Model exported successfully to '{export_path}/'\")\n",
        "    print(f\"Files created:\")\n",
        "    print(f\"  - complete_model.pt (full model)\")\n",
        "    print(f\"  - model_weights.pt (weights only)\")\n",
        "    print(f\"  - config.json (configuration)\")\n",
        "    print(f\"  - inference.py (inference script)\")\n",
        "\n",
        "    return export_path\n",
        "\n",
        "# Exportar modelo\n",
        "# export_path = export_model_for_production(model)"
      ],
      "metadata": {
        "id": "tCpPFMuBkgKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 19: Monitoreo y métricas avanzadas"
      ],
      "metadata": {
        "id": "YioUOYtJkjsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "class TranslationMetrics:\n",
        "    \"\"\"Clase para calcular y monitorear métricas de traducción\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metrics_history = defaultdict(list)\n",
        "\n",
        "    def calculate_bleu(self, predictions, references):\n",
        "        \"\"\"Calcula BLEU score\"\"\"\n",
        "        from sacrebleu import corpus_bleu\n",
        "        bleu = corpus_bleu(predictions, references)\n",
        "        return bleu.score\n",
        "\n",
        "    def calculate_ter(self, predictions, references):\n",
        "        \"\"\"Calcula Translation Edit Rate (TER)\"\"\"\n",
        "        # Implementación simplificada de TER\n",
        "        total_edits = 0\n",
        "        total_words = 0\n",
        "\n",
        "        for pred, ref_list in zip(predictions, references):\n",
        "            ref = ref_list[0] if isinstance(ref_list, list) else ref_list\n",
        "            pred_tokens = pred.split()\n",
        "            ref_tokens = ref.split()\n",
        "\n",
        "            # Distancia de Levenshtein normalizada\n",
        "            edits = self._levenshtein_distance(pred_tokens, ref_tokens)\n",
        "            total_edits += edits\n",
        "            total_words += len(ref_tokens)\n",
        "\n",
        "        ter = (total_edits / total_words) * 100 if total_words > 0 else 0\n",
        "        return ter\n",
        "\n",
        "    def _levenshtein_distance(self, s1, s2):\n",
        "        \"\"\"Calcula distancia de Levenshtein\"\"\"\n",
        "        if len(s1) < len(s2):\n",
        "            return self._levenshtein_distance(s2, s1)\n",
        "\n",
        "        if len(s2) == 0:\n",
        "            return len(s1)\n",
        "\n",
        "        previous_row = range(len(s2) + 1)\n",
        "        for i, c1 in enumerate(s1):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(s2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "\n",
        "        return previous_row[-1]\n",
        "\n",
        "    def calculate_inference_speed(self, model, test_texts, device):\n",
        "        \"\"\"Mide velocidad de inferencia\"\"\"\n",
        "        model.eval()\n",
        "        total_time = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text in test_texts:\n",
        "                start_time = time.time()\n",
        "                _ = model.generate_translation(text, \"eng_Latn\", \"spa_Latn\")\n",
        "                end_time = time.time()\n",
        "\n",
        "                total_time += (end_time - start_time)\n",
        "                total_tokens += len(text.split())\n",
        "\n",
        "        tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
        "        return tokens_per_second\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"Visualiza métricas\"\"\"\n",
        "        if not self.metrics_history:\n",
        "            print(\"No metrics to plot\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Plot 1: Loss history\n",
        "        if 'train_loss' in self.metrics_history:\n",
        "            axes[0, 0].plot(self.metrics_history['train_loss'], label='Train Loss')\n",
        "            if 'val_loss' in self.metrics_history:\n",
        "                axes[0, 0].plot(self.metrics_history['val_loss'], label='Val Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].set_title('Training Progress')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "        # Plot 2: BLEU scores\n",
        "        if 'bleu' in self.metrics_history:\n",
        "            axes[0, 1].plot(self.metrics_history['bleu'])\n",
        "            axes[0, 1].set_xlabel('Evaluation')\n",
        "            axes[0, 1].set_ylabel('BLEU Score')\n",
        "            axes[0, 1].set_title('BLEU Score Progress')\n",
        "\n",
        "        # Plot 3: TER scores\n",
        "        if 'ter' in self.metrics_history:\n",
        "            axes[1, 0].plot(self.metrics_history['ter'])\n",
        "            axes[1, 0].set_xlabel('Evaluation')\n",
        "            axes[1, 0].set_ylabel('TER (%)')\n",
        "            axes[1, 0].set_title('Translation Edit Rate')\n",
        "\n",
        "        # Plot 4: Inference speed\n",
        "        if 'speed' in self.metrics_history:\n",
        "            axes[1, 1].plot(self.metrics_history['speed'])\n",
        "            axes[1, 1].set_xlabel('Evaluation')\n",
        "            axes[1, 1].set_ylabel('Tokens/Second')\n",
        "            axes[1, 1].set_title('Inference Speed')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('translation_metrics.png')\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate_model_complete(self, model, test_pairs, device):\n",
        "        \"\"\"Evaluación completa del modelo\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"COMPLETE MODEL EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        predictions = []\n",
        "        references = []\n",
        "        source_texts = []\n",
        "\n",
        "        # Generar traducciones\n",
        "        print(\"\\nGenerating translations...\")\n",
        "        for source, target, src_lang, tgt_lang in tqdm(test_pairs):\n",
        "            translation = model.generate_translation(source, src_lang, tgt_lang)\n",
        "            predictions.append(translation)\n",
        "            references.append([target])\n",
        "            source_texts.append(source)\n",
        "\n",
        "        # Calcular métricas\n",
        "        bleu_score = self.calculate_bleu(predictions, references)\n",
        "        ter_score = self.calculate_ter(predictions, references)\n",
        "        speed = self.calculate_inference_speed(model, source_texts[:10], device)\n",
        "\n",
        "        # Guardar en historial\n",
        "        self.metrics_history['bleu'].append(bleu_score)\n",
        "        self.metrics_history['ter'].append(ter_score)\n",
        "        self.metrics_history['speed'].append(speed)\n",
        "\n",
        "        # Imprimir resultados\n",
        "        print(f\"\\n📊 EVALUATION RESULTS:\")\n",
        "        print(f\"  • BLEU Score: {bleu_score:.2f}\")\n",
        "        print(f\"  • TER Score: {ter_score:.2f}%\")\n",
        "        print(f\"  • Inference Speed: {speed:.1f} tokens/second\")\n",
        "\n",
        "        # Ejemplos de traducción\n",
        "        print(f\"\\n📝 SAMPLE TRANSLATIONS:\")\n",
        "        for i in range(min(3, len(predictions))):\n",
        "            print(f\"\\n  Example {i+1}:\")\n",
        "            print(f\"    Source: {source_texts[i]}\")\n",
        "            print(f\"    Reference: {references[i][0]}\")\n",
        "            print(f\"    Generated: {predictions[i]}\")\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'bleu': bleu_score,\n",
        "            'ter': ter_score,\n",
        "            'speed': speed,\n",
        "            'predictions': predictions,\n",
        "            'references': references\n",
        "        }\n",
        "\n",
        "# Usar métricas\n",
        "metrics = TranslationMetrics()\n",
        "# results = metrics.evaluate_model_complete(model, test_pairs, device)\n",
        "# metrics.plot_metrics()"
      ],
      "metadata": {
        "id": "TqlENjFako08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 20: Script principal de ejecución"
      ],
      "metadata": {
        "id": "HbAIqJEZkqix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_training_pipeline():\n",
        "    \"\"\"Pipeline completo de entrenamiento\"\"\"\n",
        "\n",
        "    print(\"🚀 Starting Hybrid NLLB-ByT5 Translation Model Training Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Configuración\n",
        "    print(\"\\n📋 Step 1: Setting up configuration...\")\n",
        "    config = HybridTranslationConfig()\n",
        "    config.batch_size = 4\n",
        "    config.num_epochs = 3\n",
        "    config.learning_rate = 5e-5\n",
        "    print(f\"  • Batch size: {config.batch_size}\")\n",
        "    print(f\"  • Epochs: {config.num_epochs}\")\n",
        "    print(f\"  • Learning rate: {config.learning_rate}\")\n",
        "\n",
        "    # 2. Preparar datos\n",
        "    print(\"\\n📊 Step 2: Preparing data...\")\n",
        "    train_pairs, val_pairs = prepare_training_data()\n",
        "    print(f\"  • Training samples: {len(train_pairs)}\")\n",
        "    print(f\"  • Validation samples: {len(val_pairs)}\")\n",
        "\n",
        "    # 3. Inicializar modelo\n",
        "    print(\"\\n🤖 Step 3: Initializing hybrid model...\")\n",
        "    model = OptimizedHybridModel(config)\n",
        "    model = model.to(device)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  • Total parameters: {total_params:,}\")\n",
        "    print(f\"  • Device: {device}\")\n",
        "\n",
        "    # 4. Entrenar modelo\n",
        "    print(\"\\n🏋️ Step 4: Training model...\")\n",
        "    model = train_with_optimization(model, train_pairs, val_pairs, config, device)\n",
        "\n",
        "    # 5. Evaluar modelo\n",
        "    print(\"\\n📈 Step 5: Evaluating model...\")\n",
        "    metrics = TranslationMetrics()\n",
        "    test_pairs = val_pairs[:5]  # Usar subset para evaluación rápida\n",
        "    results = metrics.evaluate_model_complete(model, test_pairs, device)\n",
        "\n",
        "    # 6. Guardar modelo\n",
        "    print(\"\\n💾 Step 6: Saving model...\")\n",
        "    export_path = export_model_for_production(model, 'final_hybrid_model')\n",
        "\n",
        "    print(\"\\n✅ Pipeline completed successfully!\")\n",
        "    print(f\"  • Model saved to: {export_path}\")\n",
        "    print(f\"  • Final BLEU score: {results['bleu']:.2f}\")\n",
        "\n",
        "    return model, results\n",
        "\n",
        "# Ejecutar pipeline completo\n",
        "# trained_model, evaluation_results = main_training_pipeline()"
      ],
      "metadata": {
        "id": "zB8flafJks3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 21: Instrucciones de uso y próximos pasos"
      ],
      "metadata": {
        "id": "RnLMUOBZktic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════╗\n",
        "║          HYBRID NLLB-ByT5 TRANSLATION MODEL - USER GUIDE         ║\n",
        "╚══════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "📚 CÓMO USAR ESTE NOTEBOOK:\n",
        "\n",
        "1️⃣ PREPARACIÓN:\n",
        "   - Asegúrate de tener GPU disponible (recomendado)\n",
        "   - Instala todas las dependencias (Celda 1)\n",
        "\n",
        "2️⃣ DATOS DE ENTRENAMIENTO:\n",
        "   - Modifica la función prepare_training_data() en Celda 6\n",
        "   - Formato: (texto_origen, texto_destino, idioma_origen, idioma_destino)\n",
        "   - Para NLLB, usa códigos como: 'eng_Latn', 'spa_Latn', etc.\n",
        "\n",
        "3️⃣ CONFIGURACIÓN:\n",
        "   - Ajusta hiperparámetros en HybridTranslationConfig (Celda 3)\n",
        "   - Batch size, learning rate, epochs, etc.\n",
        "\n",
        "4️⃣ ENTRENAMIENTO:\n",
        "   - Ejecuta main_training_pipeline() (Celda 20) para proceso completo\n",
        "   - O entrena paso a paso ejecutando celdas individuales\n",
        "\n",
        "5️⃣ AGREGAR NUEVOS IDIOMAS:\n",
        "   - Usa add_new_language_support() (Celda 12)\n",
        "   - Proporciona pares de entrenamiento para el nuevo idioma\n",
        "\n",
        "6️⃣ INFERENCIA:\n",
        "   - Usa model.generate_translation() para traducir\n",
        "   - O usa interactive_translation() para interfaz interactiva\n",
        "\n",
        "📊 IDIOMAS SOPORTADOS POR NLLB-200:\n",
        "   - 200+ idiomas incluyendo:\n",
        "     • Principales: Inglés, Español, Francés, Alemán, Chino, etc.\n",
        "     • Regionales: Catalán, Gallego, Euskera, etc.\n",
        "     • Minoritarios: Muchos idiomas con pocos recursos\n",
        "\n",
        "   Lista completa: https://github.com/facebookresearch/fairseq/tree/nllb\n",
        "\n",
        "⚙️ PERSONALIZACIÓN:\n",
        "   - Puedes cambiar el modelo NLLB base por versiones más grandes\n",
        "   - Ajusta el modelo ByT5 (small, base, large)\n",
        "   - Modifica las capas de fusión según tus necesidades\n",
        "\n",
        "⚠️ CONSIDERACIONES:\n",
        "   - El modelo distilled NLLB-200 usa ~2.4GB de VRAM\n",
        "   - ByT5-small usa ~1.2GB adicionales\n",
        "   - Con batch_size=4, necesitas ~8GB de VRAM mínimo\n",
        "   - Para modelos más grandes, ajusta batch_size y gradient_accumulation\n",
        "\n",
        "💡 PRÓXIMOS PASOS:\n",
        "   1. Entrenar con más datos (mínimo 10k pares por idioma)\n",
        "   2. Fine-tuning específico por dominio\n",
        "   3. Implementar beam search para mejor calidad\n",
        "   4. Agregar post-procesamiento específico por idioma\n",
        "   5. Crear API REST para servir el modelo\n",
        "\n",
        "📧 SOPORTE:\n",
        "   Si tienes problemas, verifica:\n",
        "   - Versiones de librerías\n",
        "   - Memoria GPU disponible\n",
        "   - Formato correcto de datos\n",
        "   - Códigos de idioma válidos\n",
        "\n",
        "¡Buena suerte con tu modelo de traducción multilingüe! 🌍🚀\n",
        "\"\"\")\n",
        "\n",
        "# Verificar estado final\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ESTADO DEL SISTEMA:\")\n",
        "print(f\"  • GPU disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  • GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  • VRAM libre: {torch.cuda.mem_get_info()[0]/1e9:.2f} GB\")\n",
        "print(f\"  • Modelos cargados: ✓\")\n",
        "print(f\"  • Listo para entrenamiento: ✓\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "ppiXahW5kwqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}